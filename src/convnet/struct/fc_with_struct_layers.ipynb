{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T02:29:50.571863Z",
     "start_time": "2025-04-25T02:29:50.331597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from convnet.data_loader import CIFAR_10_DataLoader\n",
    "from convnet.struct.layers import Linear, ReLU\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "707511d2d36a260d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T02:29:52.505624Z",
     "start_time": "2025-04-25T02:29:52.252444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_directory = '/Users/subhojit/Downloads/cifar-10-batches-py'\n",
    "cdl = CIFAR_10_DataLoader()\n",
    "xtrain_data, ytrain_data, Xtest, ytest = cdl.load_cifar_10_dataset(file_directory)\n",
    "\n",
    "xtrain_data = xtrain_data.astype('float32') / 255.0\n",
    "Xtest = Xtest.astype('float32') / 255.0\n",
    "\n",
    "# np.random.shuffle(Xtrain)\n",
    "n1 = int(0.8 * len(xtrain_data))\n",
    "Xtrain = xtrain_data[:n1]\n",
    "ytrain = ytrain_data[:n1]\n",
    "Xdev = xtrain_data[n1:]\n",
    "ydev = ytrain_data[n1:]\n",
    "\n",
    "num_classes = len(set(ytrain))"
   ],
   "id": "585ba6952a83e49c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T02:30:32.920698Z",
     "start_time": "2025-04-25T02:30:32.910445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data preparation\n",
    "np.random.seed(231)\n",
    "std_dev = 1e-2\n",
    "n_hidden = 200\n",
    "\n",
    "layers = [\n",
    "    Linear(3072, n_hidden, std_dev=std_dev),                ReLU(),\n",
    "    Linear(n_hidden, n_hidden, std_dev=std_dev),            ReLU(),\n",
    "    Linear(n_hidden, n_hidden, std_dev=std_dev),            ReLU(),\n",
    "    Linear(n_hidden, num_classes, std_dev=std_dev)\n",
    "]\n",
    "\n",
    "params = [p for layer in layers for p in layer.parameters()]\n",
    "# for layer in layers:\n",
    "#     for p in layer.parameters():\n",
    "#         params.append(p)\n",
    "\n",
    "print(sum(par.size for par in params))"
   ],
   "id": "38ec697a418a9784",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697010\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T02:31:52.712172Z",
     "start_time": "2025-04-25T02:31:52.708203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax_loss(scores, y):\n",
    "    epsilon = 1e-12\n",
    "    num_examples = scores.shape[0]\n",
    "    shifted_scores = scores - np.max(scores, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(shifted_scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    correct_log_probs = -np.log(probs[np.arange(num_examples), y] + epsilon)\n",
    "    loss = np.mean(correct_log_probs)\n",
    "    return loss\n",
    "\n",
    "def softmax_numpy(x, axis=1):\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)"
   ],
   "id": "eff86155bf6f14d3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_iterations = 1000\n",
    "batch_size = 128\n",
    "lossi = []\n",
    "Hs = []\n",
    "\n",
    "rev_layers = layers[::-1]\n",
    "\n",
    "for i in range(max_iterations):\n",
    "\n",
    "    #mini batch\n",
    "    ix = np.random.randint(0, Xtrain.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtrain[ix], ytrain[ix]\n",
    "\n",
    "    #farward pass\n",
    "    x = Xb\n",
    "    for layer in layers:\n",
    "        x = layer.forward(x)\n",
    "\n",
    "    logits = x\n",
    "    loss = softmax_loss(logits, Yb)\n",
    "    lossi.append(loss)\n",
    "\n",
    "    #backward pass\n",
    "    ddlogits = softmax_numpy(logits)\n",
    "    ddlogits[np.arange(Xb.shape[0]), Yb] -= 1\n",
    "    ddlogits /= Xb.shape[0]\n",
    "\n",
    "    for layer in rev_layers:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.backward(ddlogits)\n",
    "        elif isinstance(layer, ReLU):\n",
    "            layer.backward(ddlogits)\n",
    "\n",
    "\n",
    "    grads = [dW1, db1, dW2, db2]\n",
    "    # print(grads)\n",
    "    lr = 0.01\n",
    "    W1 += -lr * dW1\n",
    "    b1 += -lr * db1\n",
    "    W2 += -lr * dW2\n",
    "    b2 += -lr * db2\n",
    "    if i % 100 == 0:\n",
    "        print(f\"loss: {loss:.4f}\")"
   ],
   "id": "35c4c44403646e34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
