{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-20T11:27:41.837524Z",
     "start_time": "2025-06-20T11:27:28.071357Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "from after_pt.reward_model import RewardModel\n",
    "\n",
    "login(token='<>')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhojit/workspace/saturn/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/subhojit/workspace/saturn/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:27:45.353490Z",
     "start_time": "2025-06-20T11:27:45.350692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from after_pt import *"
   ],
   "id": "3cd6f493e95a1f7c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:27:58.365004Z",
     "start_time": "2025-06-20T11:27:46.628689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"mps\"\n",
    "model = model.to(device)"
   ],
   "id": "c695c98bac3a9262",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:28:29.518531Z",
     "start_time": "2025-06-20T11:28:29.510275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"<s>[INST] Who is Kurt Godel ? [/INST]\"\"\"\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)"
   ],
   "id": "1140cb9760c58d8b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:28:30.866874Z",
     "start_time": "2025-06-20T11:28:30.860907Z"
    }
   },
   "cell_type": "code",
   "source": "model_inputs = encodeds.to(device)\n",
   "id": "f765e6d35afc3d15",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:28:50.501567Z",
     "start_time": "2025-06-20T11:28:32.034296Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)",
   "id": "b06fa1c2bf36fff0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:29:06.647548Z",
     "start_time": "2025-06-20T11:29:06.640537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ],
   "id": "de80244e69b9601a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Kurt Godel ? [/INST] Kurt Godel (1906-1978) was a German logician and mathematician, known for his work in axiomatic set theory, proof theory, and mathematical logic. Godel is famous for his Godel's incompleteness theorems, which show that within any axiomatic system that contains basic arithmetic, there will always be true statements that cannot be proven within that system. Godel's work also introduced new definitions and methods in mathematical logic, and helped to lay the foundation for the development of modern computability theory.</s>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:30:26.994706Z",
     "start_time": "2025-06-20T11:30:26.990241Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e0eace7dad1c0db1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:03:27.332915Z",
     "start_time": "2025-06-20T14:03:15.244241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class RewardModelLite1(nn.Module):\n",
    "    def __init__(self, base_model, tokenizer):\n",
    "        super().__init__()\n",
    "        device = \"mps\"\n",
    "        self.base_model = base_model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        # scalar reward head\n",
    "        self.reward_head = nn.Linear(self.base_model.config.hidden_size, 1, device=device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        last_token_idx = attention_mask.sum(dim=1) - 1  # (B,)\n",
    "        last_token_hidden = last_hidden_state[torch.arange(last_hidden_state.size(0)), last_token_idx]  # (B, D)\n",
    "        reward = self.reward_head(last_token_hidden).squeeze(-1)  # (B,)\n",
    "        return reward\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = \"<s>[INST] What is dropout in neural networks? [/INST] Dropout is a regularization technique to switch off neuron randomly\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "reward_model = RewardModelLite1(model, tokenizer)\n",
    "reward = reward_model(**inputs)  # scalar for each example\n",
    "print(reward)\n"
   ],
   "id": "929e28cd5c09ed6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.8087], device='mps:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:29:54.966149Z",
     "start_time": "2025-06-20T14:29:54.962006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open('datas/sample.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('datas/prepare_sample.jsonl', 'w') as f:\n",
    "    for sample in data:\n",
    "        f.write(json.dumps(sample) + \"\\n\")\n",
    "\n"
   ],
   "id": "703d79e4deee90e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:34:56.909557Z",
     "start_time": "2025-06-20T14:34:56.900715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class RewardDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_length=512):\n",
    "        self.data = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                self.data.append(entry)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        chosen_input = self.tokenizer(\n",
    "            \"<s>[INST] \" + item[\"prompt\"] + \" [/INST] \" + item[\"chosen\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        rejected_input = self.tokenizer(\n",
    "            \"<s>[INST] \" + item[\"prompt\"] + \" [/INST] \" + item[\"rejected\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_input[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_attention_mask\": chosen_input[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_input_ids\": rejected_input[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_attention_mask\": rejected_input[\"attention_mask\"].squeeze(0)\n",
    "        }"
   ],
   "id": "1fb3e4cee2ac8cf0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:39:15.763626Z",
     "start_time": "2025-06-20T14:39:15.759506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pairwise_loss(chosen_reward, rejected_reward):\n",
    "    return -torch.log(torch.sigmoid(chosen_reward - rejected_reward)).mean()"
   ],
   "id": "6cf19fb50f56980e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:40:30.427695Z",
     "start_time": "2025-06-20T14:40:30.420494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = RewardDataset('datas/prepare_sample.jsonl', tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ],
   "id": "2e69fce05279452d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:47:10.417386Z",
     "start_time": "2025-06-20T14:44:35.219930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#training\n",
    "from tqdm import tqdm\n",
    "\n",
    "reward_model = RewardModelLite1(model, tokenizer).to(device)\n",
    "optimizer = torch.optim.AdamW(reward_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader):\n",
    "        chosen_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "        chosen_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "        rejected_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "        rejected_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "        chosen_reward = reward_model(chosen_ids, chosen_mask)\n",
    "        rejected_reward = reward_model(rejected_ids, rejected_mask)\n",
    "        loss = pairwise_loss(chosen_reward, rejected_reward)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1} | Average Loss: {total_loss / len(dataloader):.4f}\")\n"
   ],
   "id": "39fb28fa22aecccc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [02:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 61.12 GB, other allocations: 4.08 MB, max allowed: 61.20 GB). Tried to allocate 224.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m rejected_reward \u001B[38;5;241m=\u001B[39m reward_model(rejected_ids, rejected_mask)\n\u001B[1;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m pairwise_loss(chosen_reward, rejected_reward)\n\u001B[0;32m---> 20\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 61.12 GB, other allocations: 4.08 MB, max allowed: 61.20 GB). Tried to allocate 224.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T13:50:26.777446Z",
     "start_time": "2025-06-21T13:50:26.770910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "T = 8\n",
    "wei = torch.randn(T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ],
   "id": "a6fb493443d7ea0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9127, 0.0873, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4587, 0.2548, 0.2864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3671, 0.2052, 0.0691, 0.3586, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1384, 0.2307, 0.1311, 0.2231, 0.2767, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0618, 0.2446, 0.1010, 0.5094, 0.0152, 0.0680, 0.0000, 0.0000],\n",
       "        [0.1356, 0.0704, 0.2990, 0.0679, 0.0795, 0.1209, 0.2267, 0.0000],\n",
       "        [0.1211, 0.0943, 0.0239, 0.4070, 0.0218, 0.2151, 0.0446, 0.0721]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b1e889b4aff710e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
