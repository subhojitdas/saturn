{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-18T12:13:53.127185Z",
     "start_time": "2025-05-18T12:13:52.817806Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:13:10.668959Z",
     "start_time": "2025-05-18T12:13:08.327948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from transfer_learning.bert_plus import *"
   ],
   "id": "cf2a6de3e4a58042",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhojit/workspace/saturn/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/subhojit/workspace/saturn/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:05.214103Z",
     "start_time": "2025-05-18T12:13:56.031371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('imdb')\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n"
   ],
   "id": "6bc24886135b6d1e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:13.409656Z",
     "start_time": "2025-05-18T12:14:13.132430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenize = lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "import torch.nn as nn\n"
   ],
   "id": "423e48b9c84d03f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:14.890769Z",
     "start_time": "2025-05-18T12:14:14.504882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_tokenized = train_dataset.map(tokenize, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize, batched=True)"
   ],
   "id": "c0b1f7c4e7f6b847",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:15.783764Z",
     "start_time": "2025-05-18T12:14:15.780751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ],
   "id": "b87828f68119557f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:18.508357Z",
     "start_time": "2025-05-18T12:14:18.505615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_tokenized, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_tokenized, batch_size=batch_size)\n"
   ],
   "id": "cd7e883907afe684",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:20.799451Z",
     "start_time": "2025-05-18T12:14:20.791403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ],
   "id": "4811f402e7b3828b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:14:24.141578Z",
     "start_time": "2025-05-18T12:14:24.120041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 32\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "seq_len = 10\n",
    "learning_rate = 1e-3\n",
    "max_iter = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ],
   "id": "fa1cc101493efe18",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:22:42.888697Z",
     "start_time": "2025-05-18T12:19:09.941390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1-batch overfit\n",
    "batch = next(iter(train_loader))\n",
    "model = FrozenBERTClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(100):\n",
    "    model.train()\n",
    "    logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "    labels = batch['label'].to(device)\n",
    "    loss = criterion(logits, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())\n"
   ],
   "id": "8be695d7722014e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7024253010749817\n",
      "0.6743748188018799\n",
      "0.6236098408699036\n",
      "0.5830191969871521\n",
      "0.5770617127418518\n",
      "0.5154096484184265\n",
      "0.47969621419906616\n",
      "0.45839884877204895\n",
      "0.43265414237976074\n",
      "0.39990758895874023\n",
      "0.3860365152359009\n",
      "0.3703014552593231\n",
      "0.32913875579833984\n",
      "0.30773577094078064\n",
      "0.2952335774898529\n",
      "0.2798636555671692\n",
      "0.27078086137771606\n",
      "0.2524348795413971\n",
      "0.2135792374610901\n",
      "0.1780475527048111\n",
      "0.18396669626235962\n",
      "0.15322457253932953\n",
      "0.17389443516731262\n",
      "0.15604929625988007\n",
      "0.11936812102794647\n",
      "0.13020046055316925\n",
      "0.10692240297794342\n",
      "0.11775581538677216\n",
      "0.11042345315217972\n",
      "0.0984061062335968\n",
      "0.1510666012763977\n",
      "0.0940900593996048\n",
      "0.07862658053636551\n",
      "0.08892315626144409\n",
      "0.06174168363213539\n",
      "0.07129474729299545\n",
      "0.08127417415380478\n",
      "0.056967996060848236\n",
      "0.08312320709228516\n",
      "0.05900858715176582\n",
      "0.06378106772899628\n",
      "0.037942949682474136\n",
      "0.042060233652591705\n",
      "0.04554380476474762\n",
      "0.04330800846219063\n",
      "0.03983563929796219\n",
      "0.028131749480962753\n",
      "0.02115977555513382\n",
      "0.022705916315317154\n",
      "0.036153823137283325\n",
      "0.06058128550648689\n",
      "0.02396099641919136\n",
      "0.045042023062705994\n",
      "0.028222808614373207\n",
      "0.01614723913371563\n",
      "0.03445965796709061\n",
      "0.019134242087602615\n",
      "0.02186138741672039\n",
      "0.02359965071082115\n",
      "0.02882601134479046\n",
      "0.0312332920730114\n",
      "0.018967462703585625\n",
      "0.013167643919587135\n",
      "0.023624451830983162\n",
      "0.042616456747055054\n",
      "0.01591072976589203\n",
      "0.02415292151272297\n",
      "0.022461283951997757\n",
      "0.01064980123192072\n",
      "0.01327283214777708\n",
      "0.012615080922842026\n",
      "0.031345874071121216\n",
      "0.008650925010442734\n",
      "0.014643415808677673\n",
      "0.011228603310883045\n",
      "0.014558668248355389\n",
      "0.02453433722257614\n",
      "0.008782876655459404\n",
      "0.011157331988215446\n",
      "0.03780432790517807\n",
      "0.037245724350214005\n",
      "0.012005310505628586\n",
      "0.01803235150873661\n",
      "0.013723557814955711\n",
      "0.01823994889855385\n",
      "0.025487801060080528\n",
      "0.00979945994913578\n",
      "0.018337292596697807\n",
      "0.008951300755143166\n",
      "0.01393106672912836\n",
      "0.013272084295749664\n",
      "0.022600959986448288\n",
      "0.020222267135977745\n",
      "0.007636810187250376\n",
      "0.009912402369081974\n",
      "0.019623102620244026\n",
      "0.006964830681681633\n",
      "0.025952277705073357\n",
      "0.035401761531829834\n",
      "0.026904750615358353\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:20:10.115456Z",
     "start_time": "2025-05-18T13:06:20.562678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FrozenBERTClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} Loss: {loss.item():.4f}\")\n",
    "    step += 1"
   ],
   "id": "22b4b2904f4de0bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Loss: 0.6896\n",
      "Step 100 Loss: 0.3358\n",
      "Step 200 Loss: 0.2901\n",
      "Step 300 Loss: 0.3526\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:35:30.733993Z",
     "start_time": "2025-05-18T13:33:04.259302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy\n",
    "\n",
    "compute_accuracy(model, test_loader)"
   ],
   "id": "b205b968e33a4729",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(all_labels, all_predictions)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accuracy\n\u001B[0;32m---> 20\u001B[0m \u001B[43mcompute_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[17], line 10\u001B[0m, in \u001B[0;36mcompute_accuracy\u001B[0;34m(model, dataloader)\u001B[0m\n\u001B[1;32m      7\u001B[0m all_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m---> 10\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     11\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     12\u001B[0m     labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[0;32mIn[17], line 10\u001B[0m, in \u001B[0;36mcompute_accuracy\u001B[0;34m(model, dataloader)\u001B[0m\n\u001B[1;32m      7\u001B[0m all_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m---> 10\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     11\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     12\u001B[0m     labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1061\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[1;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[0;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1220\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1229\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finish_debugging_session:\n\u001B[1;32m   1226\u001B[0m     \u001B[38;5;66;03m# before every stop check if matplotlib modules were imported inside script code\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activate_mpl_if_needed()\n\u001B[0;32m-> 1229\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[43minfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpydev_state\u001B[49m \u001B[38;5;241m==\u001B[39m STATE_SUSPEND \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finish_debugging_session:\n\u001B[1;32m   1230\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmpl_in_use:\n\u001B[1;32m   1231\u001B[0m             \u001B[38;5;66;03m# call input hooks if only matplotlib is in use\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32666a804586c565"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
