{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from lstm_att.lstm_wo_attention import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_dir = '/Users/subhojit/datasets/sms_spam_collection'\n",
    "df = pd.read_csv(dataset_dir + \"/SMSSpamCollection\", sep='\\t', header=None, names=['label', 'text'])\n",
    "df.head()"
   ],
   "id": "803447099d73f936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['label'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ],
   "id": "cb4962bde86c379b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sms = df['text'].to_numpy()\n",
    "c = sorted(list(set(''.join(sms))))\n",
    "chars = c + ['<SOS>', '<EOS>', '<PAD>']\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "vocab_size, stoi\n"
   ],
   "id": "c534fa2b949d785e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[xi] for xi in s]\n",
    "decode = lambda l: ''.join([itos[li] for li in l])\n",
    "encode('l$qweqw')"
   ],
   "id": "5b21f8c2fb3ac892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "xtrain, xval, ytrain, yval = train_test_split(\n",
    "    df['text'].to_numpy(),\n",
    "    df['label'].to_numpy(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ],
   "id": "4befcfe6b8ea0d17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:34:29.798860Z",
     "start_time": "2025-05-13T20:34:29.795949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "\n",
    "device = \"cpu\""
   ],
   "id": "d88dda8c863e6e47",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:34:37.606729Z",
     "start_time": "2025-05-13T20:34:37.602072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_sequences(sequences):\n",
    "    pad_index = stoi['<PAD>']\n",
    "    max_len = np.max([len(s) for s in sequences])\n",
    "    padded_seq = np.full((len(sequences), max_len), pad_index, dtype=np.int32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_seq[i, :len(seq)] = seq\n",
    "    return padded_seq\n",
    "\n",
    "\n",
    "def get_batch(batch_size, split='train'):\n",
    "    data = xtrain if split == 'train' else xval\n",
    "    target = ytrain if split == 'train' else yval\n",
    "    idx = np.random.randint(0, len(data), (batch_size,))\n",
    "    x_sample = [encode(s) for s in data[idx]]\n",
    "    y_sample = target[idx]\n",
    "    xpadded = pad_sequences(x_sample)\n",
    "    xb, yb = xpadded, y_sample\n",
    "    yb = torch.from_numpy(yb)\n",
    "    yb = yb\n",
    "    # yb = torch.nn.functional.one_hot(yb - 1, num_classes=2)\n",
    "    xb = torch.from_numpy(xb)\n",
    "    x = xb.to(device, dtype=torch.long)\n",
    "    y = yb.to(device, dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "xb, yb = get_batch(batch_size)"
   ],
   "id": "7cc1cc786718e175",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:34:41.569602Z",
     "start_time": "2025-05-13T20:34:41.563792Z"
    }
   },
   "cell_type": "code",
   "source": "xb[0], yb[0]",
   "id": "847bc28cd9bb8aac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 43,  79,  71,  66,  68,  85,   2,  73,  66,  81,  81,  90,   2,  79,\n",
       "          70,  88,   2,  90,  70,  66,  83,  16,   2,  42,  80,  88,   2,  66,\n",
       "          83,  70,   2,  90,  80,  86,   2,  88,  73,  70,  83,  70,   2,  66,\n",
       "          83,  70,   2,  90,  80,  86,   2,  88,  73,  70,  79,   2,  66,  83,\n",
       "          70,   2,  88,  70,   2,  84,  70,  70,  74,  79,  72, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120,\n",
       "         120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training with LSTM with attention\n",
    "embedding_dim = 32\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "learning_rate = 1e-2\n",
    "max_iter = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "model = LSTMWithAttention(vocab_size, hidden_size, output_size, embedding_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iter):\n",
    "    xb, yb = get_batch(batch_size)\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # print(\"grad norm:\", model.fc.weight.grad.norm())\n",
    "    # print(\"Input device:\", xb.device)\n",
    "    # print(\"Output device:\", yb.device)\n",
    "    # print(\"Model device:\", next(model.parameters()).device)\n",
    "    # print(\"Logits device:\", logits.device)\n",
    "\n",
    "    # print(\"xb shape: \", xb.shape)  # should be (batch_size, seq_len)\n",
    "    # print(\"xb dtype: \", xb.dtype)\n",
    "    # print(\"yb shape: \", yb.shape)  # should be (batch_size,)\n",
    "    # print(\"yb dtype: \", yb.dtype)  # should be torch.long\n",
    "    # print(\"logits shape: \", logits.shape)  # should be (batch_size, 2)\n",
    "\n",
    "    # print(\"Loss:\", loss.item())\n",
    "    optimizer.step()\n",
    "    if step % eval_interval == 0:\n",
    "        print(f\"step {step}: train loss {loss:.4f}\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(xb)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            print(\"Confidence range:\", probs.max(dim=1).values[:10])\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            print(\"Preds: \", preds.tolist())\n",
    "            print(\"Targets: \", yb.tolist())"
   ],
   "id": "e9ee2df7069baf02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:44:20.082938Z",
     "start_time": "2025-05-13T20:44:12.514389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# one batch overfitting\n",
    "model = LSTMWithAttention(vocab_size, hidden_size, output_size, embedding_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "xb, yb = get_batch(64)\n",
    "# xb, yb = xb.to(device), yb.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "for step in range(1000):\n",
    "    # print(\"Input:\", xb.shape, xb.dtype, xb.device)\n",
    "    # print(\"Target:\", yb.shape, yb.dtype, yb.device)\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Logits grad_fn:\", logits.grad_fn)\n",
    "    print(\"Loss grad_fn:\", loss.grad_fn)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {step}, loss = {loss.item():.4f}\")\n",
    "    # with torch.no_grad():\n",
    "    #     logits = model(xb)\n",
    "    #     probs = torch.softmax(logits, dim=1)\n",
    "    #     print(\"Confidence range:\", probs.max(dim=1).values[:10])\n",
    "    #     preds = torch.argmax(logits, dim=1)\n",
    "    #     print(\"Preds: \", preds.tolist())\n",
    "    #     print(\"Targets: \", yb.tolist())\n",
    "    if step % 100 == 0:\n",
    "        print(\"Logits:\", logits[:2])"
   ],
   "id": "103d43a744808280",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 0, loss = 0.6533\n",
      "Logits: tensor([[-0.1021, -0.2255],\n",
      "        [-0.1021, -0.2255]], grad_fn=<SliceBackward0>)\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 1, loss = 0.5570\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 2, loss = 0.5405\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 3, loss = 0.4880\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 4, loss = 0.4719\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 5, loss = 0.4800\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 6, loss = 0.4836\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 7, loss = 0.4781\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 8, loss = 0.4684\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 9, loss = 0.4590\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2582e0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2582e0>\n",
      "Step 10, loss = 0.4520\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 11, loss = 0.4476\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 12, loss = 0.4448\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 13, loss = 0.4415\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 14, loss = 0.4356\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 15, loss = 0.4247\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 16, loss = 0.4072\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 17, loss = 0.3796\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 18, loss = 0.3534\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 19, loss = 0.3330\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 20, loss = 0.2945\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 21, loss = 0.3833\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 22, loss = 0.2928\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 23, loss = 0.2836\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 24, loss = 0.2835\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 25, loss = 0.2814\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 26, loss = 0.2772\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 27, loss = 0.2724\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 28, loss = 0.2670\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 29, loss = 0.2597\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 30, loss = 0.2508\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 31, loss = 0.2437\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 32, loss = 0.2380\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 33, loss = 0.2327\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 34, loss = 0.2220\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 35, loss = 0.1989\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 36, loss = 0.1674\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 37, loss = 0.3801\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 38, loss = 0.1829\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 39, loss = 0.2252\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 40, loss = 0.2112\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 41, loss = 0.1750\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 42, loss = 0.1567\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 43, loss = 0.1840\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 44, loss = 0.1820\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 45, loss = 0.1498\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 46, loss = 0.1493\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 47, loss = 0.1487\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 48, loss = 0.1480\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 49, loss = 0.1475\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 50, loss = 0.1470\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 51, loss = 0.1461\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 52, loss = 0.1447\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 53, loss = 0.1433\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 54, loss = 0.1420\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 55, loss = 0.1406\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 56, loss = 0.1325\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 57, loss = 0.0945\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 58, loss = 0.1140\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 59, loss = 0.0896\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 60, loss = 0.0835\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 61, loss = 0.0822\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 62, loss = 0.0814\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 63, loss = 0.0809\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 64, loss = 0.0806\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 65, loss = 0.0804\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2582e0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2582e0>\n",
      "Step 66, loss = 0.0802\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 67, loss = 0.0801\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 68, loss = 0.0800\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 69, loss = 0.0799\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 70, loss = 0.0797\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 71, loss = 0.0796\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 72, loss = 0.0795\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 73, loss = 0.0793\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 74, loss = 0.0792\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 75, loss = 0.0790\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 76, loss = 0.0789\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 77, loss = 0.0787\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 78, loss = 0.0785\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 79, loss = 0.0784\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 80, loss = 0.0783\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 81, loss = 0.0782\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2582e0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2582e0>\n",
      "Step 82, loss = 0.0781\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 83, loss = 0.0780\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 84, loss = 0.0779\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 85, loss = 0.0779\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 86, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 87, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 88, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 89, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34396f310>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 90, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 91, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 92, loss = 0.0778\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 93, loss = 0.0777\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 94, loss = 0.0777\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 95, loss = 0.0777\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 96, loss = 0.0777\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 97, loss = 0.0777\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 98, loss = 0.0776\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 99, loss = 0.0776\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 100, loss = 0.0776\n",
      "Logits: tensor([[ 2.0156, -1.8661],\n",
      "        [ 2.0142, -1.8644]], grad_fn=<SliceBackward0>)\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 101, loss = 0.0776\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 102, loss = 0.0776\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 103, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 104, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 105, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 106, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 107, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 108, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 109, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 110, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 111, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 112, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 113, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 114, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 115, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 116, loss = 0.0775\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 117, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 118, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 119, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 120, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 121, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 122, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 123, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 124, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 125, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 126, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 127, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 128, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 129, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2582e0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2582e0>\n",
      "Step 130, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 131, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 132, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 133, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 134, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 135, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 136, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34396f040>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34396f040>\n",
      "Step 137, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x3418a7d30>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x3418a7d30>\n",
      "Step 138, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 139, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 140, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 141, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 142, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 143, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 144, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 145, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 146, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 147, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 148, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 149, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 150, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 151, loss = 0.0774\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 152, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 153, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 154, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 155, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 156, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 157, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 158, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 159, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 160, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 161, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 162, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 163, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 164, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 165, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 166, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 167, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 168, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 169, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 170, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 171, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 172, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 173, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 174, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2582e0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2582e0>\n",
      "Step 175, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 176, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 177, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 178, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 179, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 180, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 181, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 182, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 183, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 184, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 185, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 186, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 187, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 188, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 189, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 190, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 191, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 192, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 193, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 194, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 195, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 196, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 197, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 198, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 199, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 200, loss = 0.0773\n",
      "Logits: tensor([[ 2.0433, -1.8899],\n",
      "        [ 2.0432, -1.8897]], grad_fn=<SliceBackward0>)\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 201, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 202, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 203, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 204, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 205, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 206, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 207, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 208, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 209, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 210, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 211, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 212, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 213, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 214, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 215, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 216, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 217, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 218, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 219, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2582e0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2582e0>\n",
      "Step 220, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 221, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 222, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 223, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 224, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 225, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 226, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 227, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 228, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 229, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 230, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 231, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 232, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 233, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 234, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 235, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 236, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 237, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 238, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 239, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 240, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 241, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 242, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 243, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 244, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x3418a7d30>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x3418a7d30>\n",
      "Step 245, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 246, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 247, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 248, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 249, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 250, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 251, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 252, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 253, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 254, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 255, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 256, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 257, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 258, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 259, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 260, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 261, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 262, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 263, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 264, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 265, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 266, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 267, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 268, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 269, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 270, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 271, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 272, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 273, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 274, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 275, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 276, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 277, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 278, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 279, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x34d2a8cd0>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x34d2a8cd0>\n",
      "Step 280, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 281, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 282, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 283, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 284, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 285, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 286, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 287, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 288, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 289, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 290, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 291, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 292, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 293, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 294, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 295, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 296, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x3418a7d30>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x3418a7d30>\n",
      "Step 297, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 298, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a670>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a670>\n",
      "Step 299, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 300, loss = 0.0773\n",
      "Logits: tensor([[ 2.0435, -1.8889],\n",
      "        [ 2.0434, -1.8888]], grad_fn=<SliceBackward0>)\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 301, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 302, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 303, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 304, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 305, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 306, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 307, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 308, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 309, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 310, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 311, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 312, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 313, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 314, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n",
      "Step 315, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a250>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a250>\n",
      "Step 316, loss = 0.0773\n",
      "Logits grad_fn: <AddmmBackward0 object at 0x343e2a070>\n",
      "Loss grad_fn: <NllLossBackward0 object at 0x343e2a070>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogits grad_fn:\u001B[39m\u001B[38;5;124m\"\u001B[39m, logits\u001B[38;5;241m.\u001B[39mgrad_fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoss grad_fn:\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss\u001B[38;5;241m.\u001B[39mgrad_fn)\n\u001B[0;32m---> 17\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:42:05.906509Z",
     "start_time": "2025-05-13T20:42:05.902420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad norm = {param.grad.norm():.4f}\")"
   ],
   "id": "718ce9508d686114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight: grad norm = 0.0000\n",
      "lstm.weight_ih_l0: grad norm = 0.0000\n",
      "lstm.weight_hh_l0: grad norm = 0.0000\n",
      "lstm.bias_ih_l0: grad norm = 0.0000\n",
      "lstm.bias_hh_l0: grad norm = 0.0000\n",
      "fc.weight: grad norm = 0.0000\n",
      "fc.bias: grad norm = 0.0000\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b5d49c5343cb776"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
