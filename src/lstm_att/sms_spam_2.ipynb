{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T21:52:43.129807Z",
     "start_time": "2025-05-13T21:48:23.552734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "dataset_dir = '/Users/subhojit/datasets/sms_spam_collection'\n",
    "df = pd.read_csv(dataset_dir + \"/SMSSpamCollection\", sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "chars = sorted(set(''.join(texts)))\n",
    "stoi = {ch: i+1 for i, ch in enumerate(chars)}\n",
    "stoi['<PAD>'] = 0\n",
    "vocab_size = len(stoi)\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "\n",
    "xtrain, xval, ytrain, yval = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "def pad_sequences(sequences, max_len=256):\n",
    "    padded = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    lengths = torch.zeros(len(sequences), dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq = seq[:max_len]\n",
    "        padded[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "        lengths[i] = len(seq)\n",
    "    return padded, lengths\n",
    "\n",
    "def get_batch(batch_size, split='train'):\n",
    "    x = xtrain if split == 'train' else xval\n",
    "    y = ytrain if split == 'train' else yval\n",
    "    idx = torch.randint(0, len(x), (batch_size,))\n",
    "    xb = [encode(x[i]) for i in idx]\n",
    "    yb = [y[i] for i in idx]\n",
    "    xb, lengths = pad_sequences(xb)\n",
    "    return xb, torch.tensor(yb, dtype=torch.long), lengths\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, _) = self.lstm(packed)\n",
    "        return self.fc(h_n[-1])\n",
    "        # return self.fc(packed_out[:, -1, :])\n",
    "\n",
    "device = 'mps'\n",
    "model = LSTMClassifier(vocab_size=vocab_size, embed_dim=32, hidden_dim=64, output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for step in range(1000):\n",
    "    xb, yb, lengths = get_batch(batch_size)\n",
    "    xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "\n",
    "    model.train()\n",
    "    logits = model(xb, lengths)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"ðŸ”¥ NaN in gradient of {name}\")\n",
    "                param.grad = torch.nan_to_num(param.grad, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f\"Step {step}, loss = {loss.item():.4f}\")\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        confidences = torch.softmax(logits, dim=1).max(dim=1).values\n",
    "        # print(f\"Step {step}, loss = {loss.item():.4f}\")\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f\"{name}: grad norm = {param.grad.norm():.4f}\")\n",
    "        # print(\"Preds:\", preds.tolist())\n",
    "        # print(\"Targets:\", yb.tolist())\n",
    "        # print(\"Confidences:\", confidences[:10])\n",
    "        print()\n"
   ],
   "id": "30f8c163f1927a12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 0.7318\n",
      "\n",
      "Step 1, loss = 0.5770\n",
      "Step 2, loss = 0.4510\n",
      "Step 3, loss = 0.3131\n",
      "Step 4, loss = 0.5025\n",
      "Step 5, loss = 0.2335\n",
      "Step 6, loss = 0.4168\n",
      "Step 7, loss = 0.2461\n",
      "Step 8, loss = 0.4098\n",
      "Step 9, loss = 0.3293\n",
      "Step 10, loss = 0.2666\n",
      "Step 11, loss = 0.3041\n",
      "Step 12, loss = 0.3543\n",
      "Step 13, loss = 0.2961\n",
      "Step 14, loss = 0.2607\n",
      "Step 15, loss = 0.4103\n",
      "Step 16, loss = 0.3710\n",
      "Step 17, loss = 0.2879\n",
      "Step 18, loss = 0.2469\n",
      "Step 19, loss = 0.2143\n",
      "Step 20, loss = 0.4174\n",
      "Step 21, loss = 0.3537\n",
      "Step 22, loss = 0.2369\n",
      "Step 23, loss = 0.2018\n",
      "Step 24, loss = 0.3297\n",
      "Step 25, loss = 0.1878\n",
      "Step 26, loss = 0.1536\n",
      "Step 27, loss = 0.1385\n",
      "Step 28, loss = 0.1006\n",
      "Step 29, loss = 0.1741\n",
      "Step 30, loss = 0.1868\n",
      "Step 31, loss = 0.1876\n",
      "Step 32, loss = 0.1029\n",
      "Step 33, loss = 0.0267\n",
      "Step 34, loss = 0.2399\n",
      "Step 35, loss = 0.3145\n",
      "Step 36, loss = 0.1659\n",
      "Step 37, loss = 0.0729\n",
      "Step 38, loss = 0.3802\n",
      "Step 39, loss = 0.1271\n",
      "Step 40, loss = 0.0284\n",
      "Step 41, loss = 0.1076\n",
      "Step 42, loss = 0.1930\n",
      "Step 43, loss = 0.1844\n",
      "Step 44, loss = 0.0842\n",
      "Step 45, loss = 0.0628\n",
      "Step 46, loss = 0.0948\n",
      "Step 47, loss = 0.2130\n",
      "Step 48, loss = 0.1805\n",
      "Step 49, loss = 0.1167\n",
      "Step 50, loss = 0.0969\n",
      "Step 51, loss = 0.0981\n",
      "Step 52, loss = 0.2148\n",
      "Step 53, loss = 0.0165\n",
      "Step 54, loss = 0.0917\n",
      "Step 55, loss = 0.3486\n",
      "Step 56, loss = 0.0901\n",
      "Step 57, loss = 0.2686\n",
      "Step 58, loss = 0.1235\n",
      "Step 59, loss = 0.1423\n",
      "Step 60, loss = 0.0303\n",
      "Step 61, loss = 0.1448\n",
      "Step 62, loss = 0.0656\n",
      "Step 63, loss = 0.0876\n",
      "Step 64, loss = 0.1289\n",
      "Step 65, loss = 0.2497\n",
      "Step 66, loss = 0.0750\n",
      "Step 67, loss = 0.0809\n",
      "Step 68, loss = 0.1699\n",
      "Step 69, loss = 0.0356\n",
      "Step 70, loss = 0.1783\n",
      "Step 71, loss = 0.0962\n",
      "Step 72, loss = 0.0279\n",
      "Step 73, loss = 0.1506\n",
      "Step 74, loss = 0.2227\n",
      "Step 75, loss = 0.1588\n",
      "Step 76, loss = 0.1043\n",
      "Step 77, loss = 0.0263\n",
      "Step 78, loss = 0.0319\n",
      "Step 79, loss = 0.0250\n",
      "Step 80, loss = 0.1430\n",
      "Step 81, loss = 0.0154\n",
      "Step 82, loss = 0.1670\n",
      "Step 83, loss = 0.2687\n",
      "Step 84, loss = 0.1043\n",
      "Step 85, loss = 0.0953\n",
      "Step 86, loss = 0.2470\n",
      "Step 87, loss = 0.0645\n",
      "Step 88, loss = 0.0562\n",
      "Step 89, loss = 0.0209\n",
      "Step 90, loss = 0.1867\n",
      "Step 91, loss = 0.0268\n",
      "Step 92, loss = 0.0928\n",
      "Step 93, loss = 0.0534\n",
      "Step 94, loss = 0.0835\n",
      "Step 95, loss = 0.1246\n",
      "Step 96, loss = 0.1648\n",
      "Step 97, loss = 0.0710\n",
      "Step 98, loss = 0.1976\n",
      "Step 99, loss = 0.0816\n",
      "Step 100, loss = 0.0860\n",
      "\n",
      "Step 101, loss = 0.0751\n",
      "Step 102, loss = 0.0819\n",
      "Step 103, loss = 0.1706\n",
      "Step 104, loss = 0.1500\n",
      "Step 105, loss = 0.1432\n",
      "Step 106, loss = 0.0646\n",
      "Step 107, loss = 0.0362\n",
      "Step 108, loss = 0.1784\n",
      "Step 109, loss = 0.0758\n",
      "Step 110, loss = 0.1426\n",
      "Step 111, loss = 0.0766\n",
      "Step 112, loss = 0.1246\n",
      "Step 113, loss = 0.1654\n",
      "Step 114, loss = 0.0599\n",
      "Step 115, loss = 0.0848\n",
      "Step 116, loss = 0.2363\n",
      "Step 117, loss = 0.1524\n",
      "Step 118, loss = 0.0864\n",
      "Step 119, loss = 0.0886\n",
      "Step 120, loss = 0.1101\n",
      "Step 121, loss = 0.2722\n",
      "Step 122, loss = 0.0313\n",
      "Step 123, loss = 0.1108\n",
      "Step 124, loss = 0.0522\n",
      "Step 125, loss = 0.1563\n",
      "Step 126, loss = 0.0268\n",
      "Step 127, loss = 0.1156\n",
      "Step 128, loss = 0.2963\n",
      "Step 129, loss = 0.0783\n",
      "Step 130, loss = 0.0831\n",
      "Step 131, loss = 0.0281\n",
      "Step 132, loss = 0.1291\n",
      "Step 133, loss = 0.1270\n",
      "Step 134, loss = 0.0605\n",
      "Step 135, loss = 0.1225\n",
      "Step 136, loss = 0.2000\n",
      "Step 137, loss = 0.1125\n",
      "Step 138, loss = 0.1692\n",
      "Step 139, loss = 0.1593\n",
      "Step 140, loss = 0.1876\n",
      "Step 141, loss = 0.1881\n",
      "Step 142, loss = 0.1590\n",
      "Step 143, loss = 0.1941\n",
      "Step 144, loss = 0.0693\n",
      "Step 145, loss = 0.0405\n",
      "Step 146, loss = 0.0409\n",
      "Step 147, loss = 0.0313\n",
      "Step 148, loss = 0.1938\n",
      "Step 149, loss = 0.1261\n",
      "Step 150, loss = 0.1701\n",
      "Step 151, loss = 0.0105\n",
      "Step 152, loss = 0.0930\n",
      "Step 153, loss = 0.1084\n",
      "Step 154, loss = 0.0421\n",
      "Step 155, loss = 0.0118\n",
      "Step 156, loss = 0.0126\n",
      "Step 157, loss = 0.2446\n",
      "Step 158, loss = 0.0100\n",
      "Step 159, loss = 0.2101\n",
      "Step 160, loss = 0.0730\n",
      "Step 161, loss = 0.1008\n",
      "Step 162, loss = 0.0188\n",
      "Step 163, loss = 0.1513\n",
      "Step 164, loss = 0.0910\n",
      "Step 165, loss = 0.0614\n",
      "Step 166, loss = 0.0590\n",
      "Step 167, loss = 0.0646\n",
      "Step 168, loss = 0.0231\n",
      "Step 169, loss = 0.0220\n",
      "Step 170, loss = 0.0970\n",
      "Step 171, loss = 0.0538\n",
      "Step 172, loss = 0.0184\n",
      "Step 173, loss = 0.0552\n",
      "Step 174, loss = 0.0983\n",
      "Step 175, loss = 0.0235\n",
      "Step 176, loss = 0.0816\n",
      "Step 177, loss = 0.0652\n",
      "Step 178, loss = 0.1309\n",
      "Step 179, loss = 0.1611\n",
      "Step 180, loss = 0.1152\n",
      "Step 181, loss = 0.1312\n",
      "Step 182, loss = 0.1709\n",
      "Step 183, loss = 0.0192\n",
      "Step 184, loss = 0.1711\n",
      "Step 185, loss = 0.0772\n",
      "Step 186, loss = 0.0470\n",
      "Step 187, loss = 0.0878\n",
      "Step 188, loss = 0.0334\n",
      "Step 189, loss = 0.0761\n",
      "Step 190, loss = 0.1086\n",
      "Step 191, loss = 0.0315\n",
      "Step 192, loss = 0.3115\n",
      "Step 193, loss = 0.0565\n",
      "Step 194, loss = 0.0784\n",
      "Step 195, loss = 0.3567\n",
      "Step 196, loss = 0.1223\n",
      "Step 197, loss = 0.0601\n",
      "Step 198, loss = 0.0392\n",
      "Step 199, loss = 0.1510\n",
      "Step 200, loss = 0.2664\n",
      "\n",
      "Step 201, loss = 0.0819\n",
      "Step 202, loss = 0.0305\n",
      "Step 203, loss = 0.2182\n",
      "Step 204, loss = 0.1172\n",
      "Step 205, loss = 0.0947\n",
      "Step 206, loss = 0.2275\n",
      "Step 207, loss = 0.0555\n",
      "Step 208, loss = 0.0274\n",
      "Step 209, loss = 0.0780\n",
      "Step 210, loss = 0.0624\n",
      "Step 211, loss = 0.0334\n",
      "Step 212, loss = 0.0296\n",
      "Step 213, loss = 0.0727\n",
      "Step 214, loss = 0.0273\n",
      "Step 215, loss = 0.1704\n",
      "Step 216, loss = 0.0165\n",
      "Step 217, loss = 0.0958\n",
      "Step 218, loss = 0.0089\n",
      "Step 219, loss = 0.0110\n",
      "Step 220, loss = 0.0752\n",
      "Step 221, loss = 0.0078\n",
      "Step 222, loss = 0.1626\n",
      "Step 223, loss = 0.0865\n",
      "Step 224, loss = 0.1104\n",
      "Step 225, loss = 0.0806\n",
      "Step 226, loss = 0.1856\n",
      "Step 227, loss = 0.1821\n",
      "Step 228, loss = 0.0103\n",
      "Step 229, loss = 0.0188\n",
      "Step 230, loss = 0.1252\n",
      "Step 231, loss = 0.0243\n",
      "Step 232, loss = 0.1523\n",
      "Step 233, loss = 0.1617\n",
      "Step 234, loss = 0.0199\n",
      "Step 235, loss = 0.0569\n",
      "Step 236, loss = 0.1538\n",
      "Step 237, loss = 0.1585\n",
      "Step 238, loss = 0.1223\n",
      "Step 239, loss = 0.0265\n",
      "Step 240, loss = 0.1325\n",
      "Step 241, loss = 0.0390\n",
      "Step 242, loss = 0.1139\n",
      "Step 243, loss = 0.1489\n",
      "Step 244, loss = 0.0541\n",
      "Step 245, loss = 0.0931\n",
      "Step 246, loss = 0.2698\n",
      "Step 247, loss = 0.1755\n",
      "Step 248, loss = 0.0195\n",
      "Step 249, loss = 0.0494\n",
      "Step 250, loss = 0.1767\n",
      "Step 251, loss = 0.0244\n",
      "Step 252, loss = 0.1189\n",
      "Step 253, loss = 0.1086\n",
      "Step 254, loss = 0.1404\n",
      "Step 255, loss = 0.0308\n",
      "Step 256, loss = 0.0454\n",
      "Step 257, loss = 0.0538\n",
      "Step 258, loss = 0.1118\n",
      "Step 259, loss = 0.0931\n",
      "Step 260, loss = 0.0235\n",
      "Step 261, loss = 0.0548\n",
      "Step 262, loss = 0.0415\n",
      "Step 263, loss = 0.1079\n",
      "Step 264, loss = 0.1321\n",
      "Step 265, loss = 0.0868\n",
      "Step 266, loss = 0.0099\n",
      "Step 267, loss = 0.0712\n",
      "Step 268, loss = 0.0960\n",
      "Step 269, loss = 0.1371\n",
      "Step 270, loss = 0.0778\n",
      "Step 271, loss = 0.0298\n",
      "Step 272, loss = 0.0614\n",
      "Step 273, loss = 0.0746\n",
      "Step 274, loss = 0.0828\n",
      "Step 275, loss = 0.0728\n",
      "Step 276, loss = 0.0537\n",
      "Step 277, loss = 0.1089\n",
      "Step 278, loss = 0.0849\n",
      "Step 279, loss = 0.0227\n",
      "Step 280, loss = 0.1044\n",
      "Step 281, loss = 0.0927\n",
      "Step 282, loss = 0.3341\n",
      "Step 283, loss = 0.2284\n",
      "Step 284, loss = 0.0925\n",
      "Step 285, loss = 0.0659\n",
      "Step 286, loss = 0.1284\n",
      "Step 287, loss = 0.0523\n",
      "Step 288, loss = 0.0341\n",
      "Step 289, loss = 0.0751\n",
      "Step 290, loss = 0.1218\n",
      "Step 291, loss = 0.0512\n",
      "Step 292, loss = 0.0267\n",
      "Step 293, loss = 0.1751\n",
      "Step 294, loss = 0.0110\n",
      "Step 295, loss = 0.1573\n",
      "Step 296, loss = 0.0182\n",
      "Step 297, loss = 0.0834\n",
      "Step 298, loss = 0.0990\n",
      "Step 299, loss = 0.0189\n",
      "Step 300, loss = 0.2951\n",
      "\n",
      "Step 301, loss = 0.0091\n",
      "Step 302, loss = 0.0166\n",
      "Step 303, loss = 0.0522\n",
      "Step 304, loss = 0.0353\n",
      "Step 305, loss = 0.0908\n",
      "Step 306, loss = 0.1851\n",
      "Step 307, loss = 0.0699\n",
      "Step 308, loss = 0.0628\n",
      "Step 309, loss = 0.0484\n",
      "Step 310, loss = 0.0961\n",
      "Step 311, loss = 0.0689\n",
      "Step 312, loss = 0.0485\n",
      "Step 313, loss = 0.0584\n",
      "Step 314, loss = 0.0322\n",
      "Step 315, loss = 0.0919\n",
      "Step 316, loss = 0.1127\n",
      "Step 317, loss = 0.0656\n",
      "Step 318, loss = 0.0235\n",
      "Step 319, loss = 0.0841\n",
      "Step 320, loss = 0.0455\n",
      "Step 321, loss = 0.1015\n",
      "Step 322, loss = 0.0100\n",
      "Step 323, loss = 0.0583\n",
      "Step 324, loss = 0.0225\n",
      "Step 325, loss = 0.0211\n",
      "Step 326, loss = 0.1128\n",
      "Step 327, loss = 0.0363\n",
      "Step 328, loss = 0.0154\n",
      "Step 329, loss = 0.0466\n",
      "Step 330, loss = 0.0478\n",
      "Step 331, loss = 0.0247\n",
      "Step 332, loss = 0.0207\n",
      "Step 333, loss = 0.0775\n",
      "Step 334, loss = 0.0796\n",
      "Step 335, loss = 0.0242\n",
      "Step 336, loss = 0.0692\n",
      "Step 337, loss = 0.0440\n",
      "Step 338, loss = 0.0560\n",
      "Step 339, loss = 0.1097\n",
      "Step 340, loss = 0.0407\n",
      "Step 341, loss = 0.0767\n",
      "Step 342, loss = 0.0270\n",
      "Step 343, loss = 0.0608\n",
      "Step 344, loss = 0.0241\n",
      "Step 345, loss = 0.0650\n",
      "Step 346, loss = 0.1670\n",
      "Step 347, loss = 0.0278\n",
      "Step 348, loss = 0.0600\n",
      "Step 349, loss = 0.0365\n",
      "Step 350, loss = 0.1946\n",
      "Step 351, loss = 0.1214\n",
      "Step 352, loss = 0.0414\n",
      "Step 353, loss = 0.0417\n",
      "Step 354, loss = 0.1475\n",
      "Step 355, loss = 0.0280\n",
      "Step 356, loss = 0.0866\n",
      "Step 357, loss = 0.0702\n",
      "Step 358, loss = 0.0163\n",
      "Step 359, loss = 0.0815\n",
      "Step 360, loss = 0.0994\n",
      "Step 361, loss = 0.0197\n",
      "Step 362, loss = 0.0841\n",
      "Step 363, loss = 0.0587\n",
      "Step 364, loss = 0.0907\n",
      "Step 365, loss = 0.0196\n",
      "Step 366, loss = 0.0404\n",
      "Step 367, loss = 0.0160\n",
      "Step 368, loss = 0.1266\n",
      "Step 369, loss = 0.0125\n",
      "Step 370, loss = 0.0884\n",
      "Step 371, loss = 0.0559\n",
      "Step 372, loss = 0.0974\n",
      "Step 373, loss = 0.1016\n",
      "Step 374, loss = 0.0792\n",
      "Step 375, loss = 0.0174\n",
      "Step 376, loss = 0.0920\n",
      "Step 377, loss = 0.0498\n",
      "Step 378, loss = 0.0276\n",
      "Step 379, loss = 0.0371\n",
      "Step 380, loss = 0.0826\n",
      "Step 381, loss = 0.0441\n",
      "Step 382, loss = 0.0293\n",
      "Step 383, loss = 0.2556\n",
      "Step 384, loss = 0.1117\n",
      "Step 385, loss = 0.0548\n",
      "Step 386, loss = 0.0992\n",
      "Step 387, loss = 0.0291\n",
      "Step 388, loss = 0.0131\n",
      "Step 389, loss = 0.0855\n",
      "Step 390, loss = 0.0393\n",
      "Step 391, loss = 0.1103\n",
      "Step 392, loss = 0.0301\n",
      "Step 393, loss = 0.0310\n",
      "Step 394, loss = 0.0935\n",
      "Step 395, loss = 0.1294\n",
      "Step 396, loss = 0.0577\n",
      "Step 397, loss = 0.0167\n",
      "Step 398, loss = 0.0728\n",
      "Step 399, loss = 0.0203\n",
      "Step 400, loss = 0.1538\n",
      "\n",
      "Step 401, loss = 0.1385\n",
      "Step 402, loss = 0.0078\n",
      "Step 403, loss = 0.0839\n",
      "Step 404, loss = 0.0090\n",
      "Step 405, loss = 0.0157\n",
      "Step 406, loss = 0.0460\n",
      "Step 407, loss = 0.0103\n",
      "Step 408, loss = 0.0655\n",
      "Step 409, loss = 0.0480\n",
      "Step 410, loss = 0.0554\n",
      "Step 411, loss = 0.0169\n",
      "Step 412, loss = 0.0733\n",
      "Step 413, loss = 0.0069\n",
      "Step 414, loss = 0.0125\n",
      "Step 415, loss = 0.1062\n",
      "Step 416, loss = 0.1178\n",
      "Step 417, loss = 0.0593\n",
      "Step 418, loss = 0.0460\n",
      "Step 419, loss = 0.0816\n",
      "Step 420, loss = 0.0782\n",
      "Step 421, loss = 0.0473\n",
      "Step 422, loss = 0.0314\n",
      "Step 423, loss = 0.0159\n",
      "Step 424, loss = 0.1251\n",
      "Step 425, loss = 0.0422\n",
      "Step 426, loss = 0.0498\n",
      "Step 427, loss = 0.0845\n",
      "Step 428, loss = 0.1317\n",
      "Step 429, loss = 0.0150\n",
      "Step 430, loss = 0.0555\n",
      "Step 431, loss = 0.0101\n",
      "Step 432, loss = 0.0422\n",
      "Step 433, loss = 0.0487\n",
      "Step 434, loss = 0.0062\n",
      "Step 435, loss = 0.0296\n",
      "Step 436, loss = 0.0427\n",
      "Step 437, loss = 0.0037\n",
      "Step 438, loss = 0.0039\n",
      "Step 439, loss = 0.0041\n",
      "Step 440, loss = 0.0955\n",
      "Step 441, loss = 0.0106\n",
      "Step 442, loss = 0.1719\n",
      "Step 443, loss = 0.1068\n",
      "Step 444, loss = 0.2062\n",
      "Step 445, loss = 0.0073\n",
      "Step 446, loss = 0.0425\n",
      "Step 447, loss = 0.0093\n",
      "Step 448, loss = 0.0277\n",
      "Step 449, loss = 0.0274\n",
      "Step 450, loss = 0.0451\n",
      "Step 451, loss = 0.0218\n",
      "Step 452, loss = 0.0783\n",
      "Step 453, loss = 0.0569\n",
      "Step 454, loss = 0.0236\n",
      "Step 455, loss = 0.0164\n",
      "Step 456, loss = 0.0506\n",
      "Step 457, loss = 0.0323\n",
      "Step 458, loss = 0.0286\n",
      "Step 459, loss = 0.0126\n",
      "Step 460, loss = 0.0158\n",
      "Step 461, loss = 0.0320\n",
      "Step 462, loss = 0.0485\n",
      "Step 463, loss = 0.0114\n",
      "Step 464, loss = 0.0089\n",
      "Step 465, loss = 0.0130\n",
      "Step 466, loss = 0.0047\n",
      "Step 467, loss = 0.0603\n",
      "Step 468, loss = 0.0473\n",
      "Step 469, loss = 0.0052\n",
      "Step 470, loss = 0.0165\n",
      "Step 471, loss = 0.0181\n",
      "Step 472, loss = 0.0338\n",
      "Step 473, loss = 0.1141\n",
      "Step 474, loss = 0.0511\n",
      "Step 475, loss = 0.0097\n",
      "Step 476, loss = 0.0030\n",
      "Step 477, loss = 0.0948\n",
      "Step 478, loss = 0.0112\n",
      "Step 479, loss = 0.0235\n",
      "Step 480, loss = 0.0320\n",
      "Step 481, loss = 0.0154\n",
      "Step 482, loss = 0.0428\n",
      "Step 483, loss = 0.0870\n",
      "Step 484, loss = 0.1470\n",
      "Step 485, loss = 0.0986\n",
      "Step 486, loss = 0.0345\n",
      "Step 487, loss = 0.1413\n",
      "Step 488, loss = 0.0167\n",
      "Step 489, loss = 0.0116\n",
      "Step 490, loss = 0.0659\n",
      "Step 491, loss = 0.0246\n",
      "Step 492, loss = 0.0215\n",
      "Step 493, loss = 0.0035\n",
      "Step 494, loss = 0.0074\n",
      "Step 495, loss = 0.0474\n",
      "Step 496, loss = 0.0763\n",
      "Step 497, loss = 0.0041\n",
      "Step 498, loss = 0.0029\n",
      "Step 499, loss = 0.1802\n",
      "Step 500, loss = 0.0506\n",
      "\n",
      "Step 501, loss = 0.0799\n",
      "Step 502, loss = 0.0985\n",
      "Step 503, loss = 0.0575\n",
      "Step 504, loss = 0.0911\n",
      "Step 505, loss = 0.0731\n",
      "Step 506, loss = 0.0090\n",
      "Step 507, loss = 0.0170\n",
      "Step 508, loss = 0.1004\n",
      "Step 509, loss = 0.0855\n",
      "Step 510, loss = 0.0472\n",
      "Step 511, loss = 0.0213\n",
      "Step 512, loss = 0.0562\n",
      "Step 513, loss = 0.0237\n",
      "Step 514, loss = 0.0292\n",
      "Step 515, loss = 0.0381\n",
      "Step 516, loss = 0.0653\n",
      "Step 517, loss = 0.0214\n",
      "Step 518, loss = 0.0272\n",
      "Step 519, loss = 0.0076\n",
      "Step 520, loss = 0.0237\n",
      "Step 521, loss = 0.0087\n",
      "Step 522, loss = 0.0082\n",
      "Step 523, loss = 0.0057\n",
      "Step 524, loss = 0.0030\n",
      "Step 525, loss = 0.1436\n",
      "Step 526, loss = 0.0758\n",
      "Step 527, loss = 0.0991\n",
      "Step 528, loss = 0.0052\n",
      "Step 529, loss = 0.0103\n",
      "Step 530, loss = 0.0573\n",
      "Step 531, loss = 0.1127\n",
      "Step 532, loss = 0.1008\n",
      "Step 533, loss = 0.0110\n",
      "Step 534, loss = 0.1322\n",
      "Step 535, loss = 0.0146\n",
      "Step 536, loss = 0.0060\n",
      "Step 537, loss = 0.0122\n",
      "Step 538, loss = 0.0066\n",
      "Step 539, loss = 0.0308\n",
      "Step 540, loss = 0.0835\n",
      "Step 541, loss = 0.0113\n",
      "Step 542, loss = 0.1100\n",
      "Step 543, loss = 0.0805\n",
      "Step 544, loss = 0.0737\n",
      "Step 545, loss = 0.0599\n",
      "Step 546, loss = 0.0276\n",
      "Step 547, loss = 0.2415\n",
      "Step 548, loss = 0.0588\n",
      "Step 549, loss = 0.0909\n",
      "Step 550, loss = 0.0257\n",
      "Step 551, loss = 0.0174\n",
      "Step 552, loss = 0.0348\n",
      "Step 553, loss = 0.0338\n",
      "Step 554, loss = 0.0304\n",
      "Step 555, loss = 0.0517\n",
      "Step 556, loss = 0.0140\n",
      "Step 557, loss = 0.0426\n",
      "Step 558, loss = 0.0214\n",
      "Step 559, loss = 0.0578\n",
      "Step 560, loss = 0.0115\n",
      "Step 561, loss = 0.1517\n",
      "Step 562, loss = 0.0134\n",
      "Step 563, loss = 0.0461\n",
      "Step 564, loss = 0.0066\n",
      "Step 565, loss = 0.1040\n",
      "Step 566, loss = 0.0218\n",
      "Step 567, loss = 0.0033\n",
      "Step 568, loss = 0.0039\n",
      "Step 569, loss = 0.0441\n",
      "Step 570, loss = 0.0113\n",
      "Step 571, loss = 0.0680\n",
      "Step 572, loss = 0.0100\n",
      "Step 573, loss = 0.0766\n",
      "Step 574, loss = 0.0579\n",
      "Step 575, loss = 0.0108\n",
      "Step 576, loss = 0.0071\n",
      "Step 577, loss = 0.0677\n",
      "Step 578, loss = 0.0216\n",
      "Step 579, loss = 0.0129\n",
      "Step 580, loss = 0.0563\n",
      "Step 581, loss = 0.0057\n",
      "Step 582, loss = 0.0182\n",
      "Step 583, loss = 0.0041\n",
      "Step 584, loss = 0.0776\n",
      "Step 585, loss = 0.0271\n",
      "Step 586, loss = 0.0078\n",
      "Step 587, loss = 0.0082\n",
      "Step 588, loss = 0.0178\n",
      "Step 589, loss = 0.0112\n",
      "Step 590, loss = 0.0280\n",
      "Step 591, loss = 0.0471\n",
      "Step 592, loss = 0.0093\n",
      "Step 593, loss = 0.0080\n",
      "Step 594, loss = 0.0064\n",
      "Step 595, loss = 0.0100\n",
      "Step 596, loss = 0.0237\n",
      "Step 597, loss = 0.0655\n",
      "Step 598, loss = 0.0060\n",
      "Step 599, loss = 0.0026\n",
      "Step 600, loss = 0.0034\n",
      "\n",
      "Step 601, loss = 0.0463\n",
      "Step 602, loss = 0.0556\n",
      "Step 603, loss = 0.0178\n",
      "Step 604, loss = 0.1375\n",
      "Step 605, loss = 0.0408\n",
      "Step 606, loss = 0.0134\n",
      "Step 607, loss = 0.0253\n",
      "Step 608, loss = 0.0144\n",
      "Step 609, loss = 0.0096\n",
      "Step 610, loss = 0.0432\n",
      "Step 611, loss = 0.0100\n",
      "Step 612, loss = 0.0353\n",
      "Step 613, loss = 0.0214\n",
      "Step 614, loss = 0.0044\n",
      "Step 615, loss = 0.0020\n",
      "Step 616, loss = 0.1044\n",
      "Step 617, loss = 0.0037\n",
      "Step 618, loss = 0.0526\n",
      "Step 619, loss = 0.0084\n",
      "Step 620, loss = 0.0084\n",
      "Step 621, loss = 0.0116\n",
      "Step 622, loss = 0.0951\n",
      "Step 623, loss = 0.0067\n",
      "Step 624, loss = 0.0061\n",
      "Step 625, loss = 0.0061\n",
      "Step 626, loss = 0.0061\n",
      "Step 627, loss = 0.0131\n",
      "Step 628, loss = 0.0227\n",
      "Step 629, loss = 0.0227\n",
      "Step 630, loss = 0.0210\n",
      "Step 631, loss = 0.0379\n",
      "Step 632, loss = 0.0104\n",
      "Step 633, loss = 0.0056\n",
      "Step 634, loss = 0.0044\n",
      "Step 635, loss = 0.0057\n",
      "Step 636, loss = 0.0039\n",
      "Step 637, loss = 0.0202\n",
      "Step 638, loss = 0.0026\n",
      "Step 639, loss = 0.1399\n",
      "Step 640, loss = 0.0014\n",
      "Step 641, loss = 0.0409\n",
      "Step 642, loss = 0.0036\n",
      "Step 643, loss = 0.0611\n",
      "Step 644, loss = 0.1329\n",
      "Step 645, loss = 0.0120\n",
      "Step 646, loss = 0.0020\n",
      "Step 647, loss = 0.0094\n",
      "Step 648, loss = 0.0075\n",
      "Step 649, loss = 0.0054\n",
      "Step 650, loss = 0.1139\n",
      "Step 651, loss = 0.1369\n",
      "Step 652, loss = 0.0059\n",
      "Step 653, loss = 0.0095\n",
      "Step 654, loss = 0.0170\n",
      "Step 655, loss = 0.0120\n",
      "Step 656, loss = 0.0225\n",
      "Step 657, loss = 0.0079\n",
      "Step 658, loss = 0.0407\n",
      "Step 659, loss = 0.0119\n",
      "Step 660, loss = 0.0040\n",
      "Step 661, loss = 0.1699\n",
      "Step 662, loss = 0.0028\n",
      "Step 663, loss = 0.0320\n",
      "Step 664, loss = 0.0166\n",
      "Step 665, loss = 0.0095\n",
      "Step 666, loss = 0.0650\n",
      "Step 667, loss = 0.0145\n",
      "Step 668, loss = 0.0062\n",
      "Step 669, loss = 0.0339\n",
      "Step 670, loss = 0.0509\n",
      "Step 671, loss = 0.0210\n",
      "Step 672, loss = 0.0296\n",
      "Step 673, loss = 0.0640\n",
      "Step 674, loss = 0.0595\n",
      "Step 675, loss = 0.0544\n",
      "Step 676, loss = 0.0426\n",
      "Step 677, loss = 0.0109\n",
      "Step 678, loss = 0.0261\n",
      "Step 679, loss = 0.0059\n",
      "Step 680, loss = 0.0102\n",
      "Step 681, loss = 0.0085\n",
      "Step 682, loss = 0.0775\n",
      "Step 683, loss = 0.0336\n",
      "Step 684, loss = 0.0680\n",
      "Step 685, loss = 0.0038\n",
      "Step 686, loss = 0.0050\n",
      "Step 687, loss = 0.0208\n",
      "Step 688, loss = 0.0420\n",
      "Step 689, loss = 0.0263\n",
      "Step 690, loss = 0.0874\n",
      "Step 691, loss = 0.0060\n",
      "Step 692, loss = 0.0592\n",
      "Step 693, loss = 0.0026\n",
      "Step 694, loss = 0.1180\n",
      "Step 695, loss = 0.0306\n",
      "Step 696, loss = 0.1245\n",
      "Step 697, loss = 0.0920\n",
      "Step 698, loss = 0.0736\n",
      "Step 699, loss = 0.0171\n",
      "Step 700, loss = 0.0416\n",
      "\n",
      "Step 701, loss = 0.0169\n",
      "Step 702, loss = 0.0085\n",
      "Step 703, loss = 0.0068\n",
      "Step 704, loss = 0.1658\n",
      "Step 705, loss = 0.1263\n",
      "Step 706, loss = 0.0639\n",
      "Step 707, loss = 0.0768\n",
      "Step 708, loss = 0.0917\n",
      "Step 709, loss = 0.0310\n",
      "Step 710, loss = 0.0080\n",
      "Step 711, loss = 0.0290\n",
      "Step 712, loss = 0.0937\n",
      "Step 713, loss = 0.0624\n",
      "Step 714, loss = 0.0297\n",
      "Step 715, loss = 0.0734\n",
      "Step 716, loss = 0.0871\n",
      "Step 717, loss = 0.0354\n",
      "Step 718, loss = 0.0219\n",
      "Step 719, loss = 0.0121\n",
      "Step 720, loss = 0.1318\n",
      "Step 721, loss = 0.0694\n",
      "Step 722, loss = 0.0236\n",
      "Step 723, loss = 0.0228\n",
      "Step 724, loss = 0.0090\n",
      "Step 725, loss = 0.0205\n",
      "Step 726, loss = 0.0063\n",
      "Step 727, loss = 0.0064\n",
      "Step 728, loss = 0.0174\n",
      "Step 729, loss = 0.0034\n",
      "Step 730, loss = 0.0078\n",
      "Step 731, loss = 0.0026\n",
      "Step 732, loss = 0.0300\n",
      "Step 733, loss = 0.0081\n",
      "Step 734, loss = 0.0073\n",
      "Step 735, loss = 0.0150\n",
      "Step 736, loss = 0.0467\n",
      "Step 737, loss = 0.0056\n",
      "Step 738, loss = 0.0074\n",
      "Step 739, loss = 0.0156\n",
      "Step 740, loss = 0.0094\n",
      "Step 741, loss = 0.0035\n",
      "Step 742, loss = 0.0133\n",
      "Step 743, loss = 0.1194\n",
      "Step 744, loss = 0.0012\n",
      "Step 745, loss = 0.1144\n",
      "Step 746, loss = 0.0133\n",
      "Step 747, loss = 0.1822\n",
      "Step 748, loss = 0.0016\n",
      "Step 749, loss = 0.0186\n",
      "Step 750, loss = 0.0159\n",
      "Step 751, loss = 0.0069\n",
      "Step 752, loss = 0.0299\n",
      "Step 753, loss = 0.0715\n",
      "Step 754, loss = 0.0267\n",
      "Step 755, loss = 0.0619\n",
      "Step 756, loss = 0.0376\n",
      "Step 757, loss = 0.0113\n",
      "Step 758, loss = 0.0244\n",
      "Step 759, loss = 0.0059\n",
      "Step 760, loss = 0.0114\n",
      "Step 761, loss = 0.0268\n",
      "Step 762, loss = 0.0195\n",
      "Step 763, loss = 0.2137\n",
      "Step 764, loss = 0.0745\n",
      "Step 765, loss = 0.0722\n",
      "Step 766, loss = 0.0118\n",
      "Step 767, loss = 0.0601\n",
      "Step 768, loss = 0.0067\n",
      "Step 769, loss = 0.0231\n",
      "Step 770, loss = 0.0192\n",
      "Step 771, loss = 0.0137\n",
      "Step 772, loss = 0.0291\n",
      "Step 773, loss = 0.0695\n",
      "Step 774, loss = 0.0113\n",
      "Step 775, loss = 0.0141\n",
      "Step 776, loss = 0.1671\n",
      "Step 777, loss = 0.0068\n",
      "Step 778, loss = 0.0222\n",
      "Step 779, loss = 0.0121\n",
      "Step 780, loss = 0.0103\n",
      "Step 781, loss = 0.0273\n",
      "Step 782, loss = 0.0097\n",
      "Step 783, loss = 0.0222\n",
      "Step 784, loss = 0.0034\n",
      "Step 785, loss = 0.0030\n",
      "Step 786, loss = 0.0472\n",
      "Step 787, loss = 0.0171\n",
      "Step 788, loss = 0.0119\n",
      "Step 789, loss = 0.0549\n",
      "Step 790, loss = 0.0163\n",
      "Step 791, loss = 0.1104\n",
      "Step 792, loss = 0.0095\n",
      "Step 793, loss = 0.0027\n",
      "Step 794, loss = 0.0080\n",
      "Step 795, loss = 0.0044\n",
      "Step 796, loss = 0.0171\n",
      "Step 797, loss = 0.0525\n",
      "Step 798, loss = 0.0048\n",
      "Step 799, loss = 0.0093\n",
      "Step 800, loss = 0.0033\n",
      "\n",
      "Step 801, loss = 0.0027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 76\u001B[0m\n\u001B[1;32m     73\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, yb)\n\u001B[1;32m     75\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 76\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# ðŸ”Ž Check & clean gradients\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, param \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mnamed_parameters():\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2def11b86d66aed1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
