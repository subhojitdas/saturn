{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T19:38:45.315085Z",
     "start_time": "2025-05-14T19:38:44.056291Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from lstm_att.lstm_wo_attention import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:38:56.973081Z",
     "start_time": "2025-05-14T19:38:47.371656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dir = '/Users/subhojit/datasets/amazon_review_polarity_csv'\n",
    "df = pd.read_csv(dataset_dir + '/train.csv')\n",
    "df.head()\n",
    "df_test = pd.read_csv(dataset_dir + '/test.csv')"
   ],
   "id": "69911d267906059d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:38:59.784911Z",
     "start_time": "2025-05-14T19:38:59.778555Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "14ec532a81605bb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   2                     Stuning even for the non-gamer  \\\n",
       "0  2              The best soundtrack ever to anything.   \n",
       "1  2                                           Amazing!   \n",
       "2  2                               Excellent Soundtrack   \n",
       "3  2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "4  2                            an absolute masterpiece   \n",
       "\n",
       "  This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^  \n",
       "0  I'm reading a lot of reviews saying that this ...                                                                                                                                                                                                                                                                                                                                                          \n",
       "1  This soundtrack is my favorite music of all ti...                                                                                                                                                                                                                                                                                                                                                          \n",
       "2  I truly like this soundtrack and I enjoy video...                                                                                                                                                                                                                                                                                                                                                          \n",
       "3  If you've played the game, you know how divine...                                                                                                                                                                                                                                                                                                                                                          \n",
       "4  I am quite sure any of you actually taking the...                                                                                                                                                                                                                                                                                                                                                          "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>Stuning even for the non-gamer</th>\n",
       "      <th>This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>I am quite sure any of you actually taking the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:39:05.642481Z",
     "start_time": "2025-05-14T19:39:05.632002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = df.iloc[:, 2].to_numpy()\n",
    "y = df.iloc[:, 0].to_numpy() - 1\n",
    "\n",
    "xtest = df_test.iloc[:, 2].to_numpy()\n",
    "ytest = df_test.iloc[:, 0].to_numpy() - 1"
   ],
   "id": "81922c1ab33688f3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:39:08.590765Z",
     "start_time": "2025-05-14T19:39:08.587451Z"
    }
   },
   "cell_type": "code",
   "source": "y[:20]",
   "id": "8090ef7b25f3baa1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:39:11.658232Z",
     "start_time": "2025-05-14T19:39:11.654988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9*len(x))\n",
    "xtrain = x[:n]\n",
    "ytrain = y[:n]\n",
    "xval = x[n:]\n",
    "yval = y[n:]"
   ],
   "id": "8fd3db1eead93342",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:41:54.590628Z",
     "start_time": "2025-05-14T19:41:54.587360Z"
    }
   },
   "cell_type": "code",
   "source": "len(xtrain), len(xval), len(xtest), len(ytest), len(ytrain), len(yval)",
   "id": "472bd3bc112ada55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3239999, 360000, 399999, 399999, 3239999, 360000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_review = np.concatenate((x, xtest))\n",
    "chars = sorted(list(set(''.join(all_review))))\n",
    "stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "stoi['<PAD>'] = 0\n",
    "vocab_size = len(stoi)\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "stoi"
   ],
   "id": "620587dca9d24085",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:39:49.046697Z",
     "start_time": "2025-05-14T19:39:49.043823Z"
    }
   },
   "cell_type": "code",
   "source": "encode('ioewfureihdjbvc')",
   "id": "30070ba0a0a7850c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90, 96, 86, 104, 87, 102, 99, 86, 90, 89, 85, 91, 83, 103, 84]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:39:51.924262Z",
     "start_time": "2025-05-14T19:39:51.898690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ],
   "id": "f3883e1f9fac872f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:39:54.008849Z",
     "start_time": "2025-05-14T19:39:53.998529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_sequences(sequences):\n",
    "    pad_index = stoi['<PAD>']\n",
    "    max_len = np.max([len(s) for s in sequences])\n",
    "    lenghts = torch.zeros(len(sequences), dtype=torch.long)\n",
    "    padded_seq = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_seq[i, :len(seq)] = torch.tensor(seq)\n",
    "        lenghts[i] = len(seq)\n",
    "    return padded_seq, lenghts\n",
    "\n",
    "\n",
    "def get_batch(batch_size, split='train'):\n",
    "    x = xtrain if split == 'train' else xval\n",
    "    y = ytrain if split == 'train' else yval\n",
    "\n",
    "    idx = torch.randint(0, len(x), (batch_size,))\n",
    "    xb = [encode(x[i]) for i in idx]\n",
    "    yb = [y[i] for i in idx]\n",
    "    xb, lengths = pad_sequences(xb)\n",
    "    yb = torch.tensor(yb, dtype=torch.long)\n",
    "    return xb, yb, lengths\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "xb, yb, lengths = get_batch(batch_size)\n",
    "lengths"
   ],
   "id": "ff4750f1aef0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([237, 202, 181, 332, 389, 665, 171, 841, 560, 186, 379, 786, 134, 424,\n",
       "        250, 942, 326, 180, 793, 873, 179, 136, 822, 755, 521, 221, 454, 451,\n",
       "        440, 400, 239, 156])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:53:13.025632Z",
     "start_time": "2025-05-14T19:48:30.784033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training with LSTM with attention\n",
    "embedding_dim = 32\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "learning_rate = 1e-3\n",
    "max_iter = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "model = LSTMWithoutAttention(vocab_size, hidden_size, output_size, embedding_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iter):\n",
    "    xb, yb, lengths = get_batch(batch_size)\n",
    "    xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "\n",
    "    logits = model(xb, lengths)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(\"grad norm:\", model.fc.weight.grad.norm())\n",
    "    # print(\"Input device:\", xb.device)\n",
    "    # print(\"Output device:\", yb.device)\n",
    "    # print(\"Model device:\", next(model.parameters()).device)\n",
    "    # print(\"Logits device:\", logits.device)\n",
    "\n",
    "    # print(\"xb shape: \", xb.shape)  # should be (batch_size, seq_len)\n",
    "    # print(\"xb dtype: \", xb.dtype)\n",
    "    # print(\"yb shape: \", yb.shape)  # should be (batch_size,)\n",
    "    # print(\"yb dtype: \", yb.dtype)  # should be torch.long\n",
    "    # print(\"logits shape: \", logits.shape)  # should be (batch_size, 2)\n",
    "\n",
    "    # print(\"Loss:\", loss.item())\n",
    "    # print(f\"step {step}: train loss {loss:.4f}\")\n",
    "    if step % eval_interval == 0:\n",
    "        print(f\"step {step}: train loss {loss:.4f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     logits = model(xb)\n",
    "        #     probs = torch.softmax(logits, dim=1)\n",
    "        #     print(\"Confidence range:\", probs.max(dim=1).values[:10])\n",
    "        #     preds = torch.argmax(logits, dim=1)\n",
    "        #     print(\"Preds: \", preds.tolist())\n",
    "        #     print(\"Targets: \", yb.tolist())\n"
   ],
   "id": "db689e8c6291c2a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.7018\n",
      "step 500: train loss 0.6696\n",
      "step 1000: train loss 0.6736\n",
      "step 1500: train loss 0.6851\n",
      "step 2000: train loss 0.5855\n",
      "step 2500: train loss 0.6858\n",
      "step 3000: train loss 0.6616\n",
      "step 3500: train loss 0.6443\n",
      "step 4000: train loss 0.4994\n",
      "step 4500: train loss 0.4710\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T20:21:57.816096Z",
     "start_time": "2025-05-14T20:00:28.348371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_validation_batch(batch_size, split='val'):\n",
    "    x = xtrain if split == 'train' else xval\n",
    "    y = ytrain if split == 'train' else yval\n",
    "\n",
    "    iter_size = len(x) // batch_size\n",
    "    for i in range(iter_size):\n",
    "        idx = torch.arange(i*batch_size, i*batch_size + batch_size)\n",
    "        xb = [encode(x[i]) for i in idx]\n",
    "        yb = [y[i] for i in idx]\n",
    "        xb, lengths = pad_sequences(xb)\n",
    "        yb = torch.tensor(yb, dtype=torch.long)\n",
    "        yield xb, yb, lengths\n",
    "\n",
    "def compute_accuracy():\n",
    "    iter_size = len(xval) // batch_size\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(iter_size):\n",
    "        for xb, yb, lengths in get_validation_batch(batch_size):\n",
    "            xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "            logits = model(xb, lengths)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += len(yb)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "val_acc = compute_accuracy()"
   ],
   "id": "4c1d2c4f97d9f1d3",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 30\u001B[0m\n\u001B[1;32m     26\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m (correct \u001B[38;5;241m/\u001B[39m total) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m val_acc \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 21\u001B[0m, in \u001B[0;36mcompute_accuracy\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m xb, yb, lengths \u001B[38;5;129;01min\u001B[39;00m get_validation_batch(batch_size):\n\u001B[1;32m     20\u001B[0m     xb, yb, lengths \u001B[38;5;241m=\u001B[39m xb\u001B[38;5;241m.\u001B[39mto(device), yb\u001B[38;5;241m.\u001B[39mto(device), lengths\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 21\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlengths\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     23\u001B[0m     correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (preds \u001B[38;5;241m==\u001B[39m yb)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/saturn/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/workspace/saturn/src/lstm_att/lstm_attention.py:25\u001B[0m, in \u001B[0;36mLSTMWithAttention.forward\u001B[0;34m(self, x, lengths)\u001B[0m\n\u001B[1;32m     23\u001B[0m last_hidden \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(batch_size, out\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m), device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(batch_size):\n\u001B[0;32m---> 25\u001B[0m     last_hidden[i] \u001B[38;5;241m=\u001B[39m \u001B[43mout\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlengths\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(last_hidden)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:42:40.385794Z",
     "start_time": "2025-05-14T19:42:30.907544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# one batch overfitting\n",
    "embedding_dim = 32\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "learning_rate = 1e-2\n",
    "max_iter = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "model = LSTMWithAttention(vocab_size, hidden_size, output_size, embedding_dim).to(device)\n",
    "\n",
    "xb, yb, lengths = get_batch(100)\n",
    "xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "for step in range(100):\n",
    "    logits = model(xb, lengths)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {step}, loss = {loss.item():.4f}\")\n",
    "    if step % 100 == 0:\n",
    "        print(\"Logits:\", logits[:2])\n"
   ],
   "id": "c7fae78dfad058ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 0.6940\n",
      "Logits: tensor([[ 0.1255, -0.0343],\n",
      "        [ 0.0820,  0.0009]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "Step 1, loss = 0.6577\n",
      "Step 2, loss = 0.6290\n",
      "Step 3, loss = 0.5918\n",
      "Step 4, loss = 0.5488\n",
      "Step 5, loss = 0.5188\n",
      "Step 6, loss = 0.4915\n",
      "Step 7, loss = 0.4208\n",
      "Step 8, loss = 0.3945\n",
      "Step 9, loss = 0.3243\n",
      "Step 10, loss = 0.3015\n",
      "Step 11, loss = 0.2447\n",
      "Step 12, loss = 0.2079\n",
      "Step 13, loss = 0.1518\n",
      "Step 14, loss = 0.1174\n",
      "Step 15, loss = 0.0901\n",
      "Step 16, loss = 0.0710\n",
      "Step 17, loss = 0.0599\n",
      "Step 18, loss = 0.0298\n",
      "Step 19, loss = 0.0604\n",
      "Step 20, loss = 0.0192\n",
      "Step 21, loss = 0.0111\n",
      "Step 22, loss = 0.0089\n",
      "Step 23, loss = 0.0108\n",
      "Step 24, loss = 0.0093\n",
      "Step 25, loss = 0.0085\n",
      "Step 26, loss = 0.0058\n",
      "Step 27, loss = 0.0040\n",
      "Step 28, loss = 0.0031\n",
      "Step 29, loss = 0.0027\n",
      "Step 30, loss = 0.0022\n",
      "Step 31, loss = 0.0017\n",
      "Step 32, loss = 0.0013\n",
      "Step 33, loss = 0.0011\n",
      "Step 34, loss = 0.0010\n",
      "Step 35, loss = 0.0009\n",
      "Step 36, loss = 0.0008\n",
      "Step 37, loss = 0.0007\n",
      "Step 38, loss = 0.0006\n",
      "Step 39, loss = 0.0005\n",
      "Step 40, loss = 0.0004\n",
      "Step 41, loss = 0.0004\n",
      "Step 42, loss = 0.0003\n",
      "Step 43, loss = 0.0003\n",
      "Step 44, loss = 0.0003\n",
      "Step 45, loss = 0.0003\n",
      "Step 46, loss = 0.0002\n",
      "Step 47, loss = 0.0002\n",
      "Step 48, loss = 0.0002\n",
      "Step 49, loss = 0.0002\n",
      "Step 50, loss = 0.0002\n",
      "Step 51, loss = 0.0002\n",
      "Step 52, loss = 0.0002\n",
      "Step 53, loss = 0.0001\n",
      "Step 54, loss = 0.0001\n",
      "Step 55, loss = 0.0001\n",
      "Step 56, loss = 0.0001\n",
      "Step 57, loss = 0.0001\n",
      "Step 58, loss = 0.0001\n",
      "Step 59, loss = 0.0001\n",
      "Step 60, loss = 0.0001\n",
      "Step 61, loss = 0.0001\n",
      "Step 62, loss = 0.0001\n",
      "Step 63, loss = 0.0001\n",
      "Step 64, loss = 0.0001\n",
      "Step 65, loss = 0.0001\n",
      "Step 66, loss = 0.0001\n",
      "Step 67, loss = 0.0001\n",
      "Step 68, loss = 0.0001\n",
      "Step 69, loss = 0.0001\n",
      "Step 70, loss = 0.0001\n",
      "Step 71, loss = 0.0001\n",
      "Step 72, loss = 0.0001\n",
      "Step 73, loss = 0.0001\n",
      "Step 74, loss = 0.0001\n",
      "Step 75, loss = 0.0001\n",
      "Step 76, loss = 0.0001\n",
      "Step 77, loss = 0.0001\n",
      "Step 78, loss = 0.0001\n",
      "Step 79, loss = 0.0001\n",
      "Step 80, loss = 0.0001\n",
      "Step 81, loss = 0.0001\n",
      "Step 82, loss = 0.0001\n",
      "Step 83, loss = 0.0001\n",
      "Step 84, loss = 0.0001\n",
      "Step 85, loss = 0.0001\n",
      "Step 86, loss = 0.0001\n",
      "Step 87, loss = 0.0001\n",
      "Step 88, loss = 0.0001\n",
      "Step 89, loss = 0.0001\n",
      "Step 90, loss = 0.0001\n",
      "Step 91, loss = 0.0001\n",
      "Step 92, loss = 0.0001\n",
      "Step 93, loss = 0.0001\n",
      "Step 94, loss = 0.0001\n",
      "Step 95, loss = 0.0001\n",
      "Step 96, loss = 0.0001\n",
      "Step 97, loss = 0.0001\n",
      "Step 98, loss = 0.0001\n",
      "Step 99, loss = 0.0001\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xb, yb = get_batch(4)\n",
    "print(\"Target classes in batch:\", yb.tolist())\n",
    "assert 0 in yb and 1 in yb, \"Need both classes for learning\""
   ],
   "id": "369bc9d49e2e691",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T18:12:48.094441Z",
     "start_time": "2025-05-13T18:12:48.091191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.device)"
   ],
   "id": "fe5ff2d7c936f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight cpu\n",
      "lstm.weight_ih_l0 cpu\n",
      "lstm.weight_hh_l0 cpu\n",
      "lstm.bias_ih_l0 cpu\n",
      "lstm.bias_hh_l0 cpu\n",
      "fc.weight cpu\n",
      "fc.bias cpu\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f8e43081be91789",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(emb_dim * max_seq_len, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (B, T, D)\n",
    "        return self.net(x)"
   ],
   "id": "6d63fb1ae338b9b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xb, yb = get_batch(10)\n",
    "# xb = xb.to(\"cpu\", dtype=torch.long)\n",
    "# yb = yb.to(\"cpu\", dtype=torch.long)\n",
    "\n",
    "model = MLPClassifier(vocab_size, embedding_dim, hidden_size, 2, xb.shape[1])\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "for step in range(200):\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    print(f\"Step {step}, loss = {loss.item():.4f}, preds = {preds.tolist()}, targets = {yb.tolist()}\")\n"
   ],
   "id": "d2241d6f7cbb4cd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_validation_batch(batch_size, split='val'):\n",
    "    x = xtrain if split == 'train' else xval\n",
    "    y = ytrain if split == 'train' else yval\n",
    "\n",
    "    iter_size = len(x) // batch_size\n",
    "    for i in range(iter_size):\n",
    "        idx = torch.arange(i*batch_size, i*batch_size + batch_size)\n",
    "        xb = [encode(x[i]) for i in idx]\n",
    "        yb = [y[i] for i in idx]\n",
    "        xb, lengths = pad_sequences(xb)\n",
    "        yb = torch.tensor(yb, dtype=torch.long)\n",
    "        yield xb, yb, lengths\n",
    "\n",
    "def compute_accuracy():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb, lengths in get_validation_batch(batch_size):\n",
    "        xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "        logits = model(xb, lengths)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += len(yb)\n",
    "        # print(\"Preds:   \", preds.tolist())\n",
    "        # print(\"Targets: \", yb.tolist())\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "val_acc = compute_accuracy()"
   ],
   "id": "7dee44bb0305e968",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
