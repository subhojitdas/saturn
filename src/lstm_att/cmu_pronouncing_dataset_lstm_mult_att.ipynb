{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.049321Z",
     "start_time": "2025-05-17T09:11:30.898973Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from lstm_att.lstm_multiplicative_att import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.114042Z",
     "start_time": "2025-05-17T09:11:32.052030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dir = '/Users/subhojit/datasets/cmu_pronouncing_dict'\n",
    "dict_file = open(dataset_dir + '/cmudict.dict', 'r')\n",
    "\n",
    "words = []\n",
    "phonics_list = []\n",
    "with dict_file as f:\n",
    "    for line in f:\n",
    "        phonics = line.rstrip('\\n').split(\" \")\n",
    "        words.append(phonics[0])\n",
    "        phonics_list.append(' '.join(phonics[1:]))\n",
    "\n",
    "data = pd.DataFrame({'words': words, 'phonics': phonics_list})\n",
    "data.head()\n"
   ],
   "id": "2afff795d82ae7c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     words    phonics\n",
       "0    'bout    B AW1 T\n",
       "1   'cause    K AH0 Z\n",
       "2  'course  K AO1 R S\n",
       "3    'cuse  K Y UW1 Z\n",
       "4      'em      AH0 M"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>phonics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'bout</td>\n",
       "      <td>B AW1 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'cause</td>\n",
       "      <td>K AH0 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'course</td>\n",
       "      <td>K AO1 R S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'cuse</td>\n",
       "      <td>K Y UW1 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'em</td>\n",
       "      <td>AH0 M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.228900Z",
     "start_time": "2025-05-17T09:11:32.219837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = data.sample(data.shape[0]).reset_index(drop=True)\n",
    "df"
   ],
   "id": "e9976df06d9929d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 words                               phonics\n",
       "0       dramatizations  D R AE2 M AH0 T AH0 Z EY1 SH AH0 N Z\n",
       "1              leath's                            L IY1 TH S\n",
       "2                bietz                             B IY1 T S\n",
       "3              rasnick                       R AE1 S N IH0 K\n",
       "4             cotton's                       K AA1 T AH0 N Z\n",
       "...                ...                                   ...\n",
       "135005           henne                              HH EH1 N\n",
       "135006         trippel                       T R IH1 P AH0 L\n",
       "135007    additives(2)                   AE1 D IH0 T IH0 V Z\n",
       "135008       willamina               W IH0 L AH0 M AY1 N AH0\n",
       "135009           kills                             K IH1 L Z\n",
       "\n",
       "[135010 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>phonics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dramatizations</td>\n",
       "      <td>D R AE2 M AH0 T AH0 Z EY1 SH AH0 N Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leath's</td>\n",
       "      <td>L IY1 TH S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bietz</td>\n",
       "      <td>B IY1 T S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rasnick</td>\n",
       "      <td>R AE1 S N IH0 K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cotton's</td>\n",
       "      <td>K AA1 T AH0 N Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135005</th>\n",
       "      <td>henne</td>\n",
       "      <td>HH EH1 N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135006</th>\n",
       "      <td>trippel</td>\n",
       "      <td>T R IH1 P AH0 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135007</th>\n",
       "      <td>additives(2)</td>\n",
       "      <td>AE1 D IH0 T IH0 V Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135008</th>\n",
       "      <td>willamina</td>\n",
       "      <td>W IH0 L AH0 M AY1 N AH0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135009</th>\n",
       "      <td>kills</td>\n",
       "      <td>K IH1 L Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135010 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.266752Z",
     "start_time": "2025-05-17T09:11:32.246637Z"
    }
   },
   "cell_type": "code",
   "source": "df[~df['words'].apply(lambda x: x.isalnum())]",
   "id": "6272161bd7feecd3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               words                  phonics\n",
       "1            leath's               L IY1 TH S\n",
       "4           cotton's          K AA1 T AH0 N Z\n",
       "12        strieker's        S T R IY1 K ER0 Z\n",
       "28            kobe's            K OW1 B EY0 Z\n",
       "30         saddam(2)            S AH0 D AA1 M\n",
       "...              ...                      ...\n",
       "134971  earpieces(2)      IY1 R P IY0 S IH0 Z\n",
       "134980      farmers'          F AA1 R M ER0 Z\n",
       "134992      cache(2)             K AE0 SH EY1\n",
       "135001  directory(4)  D IH0 R EH1 K T ER0 IY0\n",
       "135007  additives(2)      AE1 D IH0 T IH0 V Z\n",
       "\n",
       "[17621 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>phonics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leath's</td>\n",
       "      <td>L IY1 TH S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cotton's</td>\n",
       "      <td>K AA1 T AH0 N Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>strieker's</td>\n",
       "      <td>S T R IY1 K ER0 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>kobe's</td>\n",
       "      <td>K OW1 B EY0 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>saddam(2)</td>\n",
       "      <td>S AH0 D AA1 M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134971</th>\n",
       "      <td>earpieces(2)</td>\n",
       "      <td>IY1 R P IY0 S IH0 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134980</th>\n",
       "      <td>farmers'</td>\n",
       "      <td>F AA1 R M ER0 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134992</th>\n",
       "      <td>cache(2)</td>\n",
       "      <td>K AE0 SH EY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135001</th>\n",
       "      <td>directory(4)</td>\n",
       "      <td>D IH0 R EH1 K T ER0 IY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135007</th>\n",
       "      <td>additives(2)</td>\n",
       "      <td>AE1 D IH0 T IH0 V Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17621 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.327020Z",
     "start_time": "2025-05-17T09:11:32.309937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['phonics'] = '<sos> ' + df['phonics'] + ' <eos>'\n",
    "df.head()"
   ],
   "id": "59482560c83cd8fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            words                                           phonics\n",
       "0  dramatizations  <sos> D R AE2 M AH0 T AH0 Z EY1 SH AH0 N Z <eos>\n",
       "1         leath's                            <sos> L IY1 TH S <eos>\n",
       "2           bietz                             <sos> B IY1 T S <eos>\n",
       "3         rasnick                       <sos> R AE1 S N IH0 K <eos>\n",
       "4        cotton's                       <sos> K AA1 T AH0 N Z <eos>"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>phonics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dramatizations</td>\n",
       "      <td>&lt;sos&gt; D R AE2 M AH0 T AH0 Z EY1 SH AH0 N Z &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leath's</td>\n",
       "      <td>&lt;sos&gt; L IY1 TH S &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bietz</td>\n",
       "      <td>&lt;sos&gt; B IY1 T S &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rasnick</td>\n",
       "      <td>&lt;sos&gt; R AE1 S N IH0 K &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cotton's</td>\n",
       "      <td>&lt;sos&gt; K AA1 T AH0 N Z &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.365074Z",
     "start_time": "2025-05-17T09:11:32.361274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = df['words'].tolist()\n",
    "phonics = df['phonics'].tolist()"
   ],
   "id": "6a6efa91f8b7bc62",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.380357Z",
     "start_time": "2025-05-17T09:11:32.378124Z"
    }
   },
   "cell_type": "code",
   "source": "words[:10] , phonics[:10]",
   "id": "9c2ab3ba1eb9241b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['dramatizations',\n",
       "  \"leath's\",\n",
       "  'bietz',\n",
       "  'rasnick',\n",
       "  \"cotton's\",\n",
       "  'irregulars',\n",
       "  'euchred',\n",
       "  'treasonous',\n",
       "  'compiler',\n",
       "  'serfs'],\n",
       " ['<sos> D R AE2 M AH0 T AH0 Z EY1 SH AH0 N Z <eos>',\n",
       "  '<sos> L IY1 TH S <eos>',\n",
       "  '<sos> B IY1 T S <eos>',\n",
       "  '<sos> R AE1 S N IH0 K <eos>',\n",
       "  '<sos> K AA1 T AH0 N Z <eos>',\n",
       "  '<sos> IH2 R EH1 G Y AH0 L ER0 Z <eos>',\n",
       "  '<sos> Y UW1 K ER0 D <eos>',\n",
       "  '<sos> T R IY1 Z AH0 N AH2 S <eos>',\n",
       "  '<sos> K AH0 M P AY1 L ER0 <eos>',\n",
       "  '<sos> S ER1 F S <eos>'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.421837Z",
     "start_time": "2025-05-17T09:11:32.401944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = [w.lower() for w in words]\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {ch:i+1 for i, ch in enumerate(chars)}\n",
    "stoi['<pad>'] = 0\n",
    "vocab_size = len(stoi)\n",
    "encode = lambda s: [stoi[ch] for ch in s.lower()]\n",
    "\n",
    "encode('Adhasdja1wasj')\n"
   ],
   "id": "236f3993987ed7a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 13, 17, 10, 28, 13, 19, 10, 6, 32, 10, 28, 19]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.617587Z",
     "start_time": "2025-05-17T09:11:32.438555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "target_seq = [p.split(' ') for p in phonics]\n",
    "target_seq = [item for nested_list in target_seq for item in nested_list]\n",
    "target_seq = list(set(target_seq))\n",
    "phoneme_vocab_size = len(target_seq)\n",
    "\n",
    "ptoi = {ph: i for i, ph in enumerate(target_seq)}\n",
    "encode_p = lambda p: ptoi[p]\n",
    "encode_phonemes = lambda p: [ptoi[ph] for ph in p]\n",
    "\n",
    "encode_phonemes(['ER0']), encode_p('ER0')\n"
   ],
   "id": "dbfb33d0c18318de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([34], 34)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.627439Z",
     "start_time": "2025-05-17T09:11:32.624501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "xtrain = words[:n1]\n",
    "ytrain = phonics[:n1]\n",
    "xval = words[n1:n2]\n",
    "yval = phonics[n1:n2]\n",
    "xtest = words[n2:]\n",
    "ytest = phonics[n2:]\n",
    "\n",
    "# xtrain = torch.tensor([encode(x) for x in xtrain], dtype=torch.long)\n",
    "# ytrain = torch.tensor([encode_phonemes(ph) for ph in ytrain], dtype=torch.long)\n",
    "# xval = torch.tensor([encode(x) for x in xval], dtype=torch.long)\n",
    "# yval = torch.tensor([encode_phonemes(ph) for ph in yval], dtype=torch.long)\n",
    "# xtest = torch.tensor([encode(x) for x in xtest], dtype=torch.long)\n",
    "# ytest = torch.tensor([encode_phonemes(ph) for ph in ytest], dtype=torch.long)\n"
   ],
   "id": "4a2387bbc2f0fe77",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.641809Z",
     "start_time": "2025-05-17T09:11:32.634633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_sequences(sequences):\n",
    "    pad_index = stoi['<pad>']\n",
    "    max_len = np.max([len(s) for s in sequences])\n",
    "    lenghts = torch.zeros(len(sequences), dtype=torch.long)\n",
    "    padded_seq = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_seq[i, :len(seq)] = torch.tensor(seq)\n",
    "        lenghts[i] = len(seq)\n",
    "    return padded_seq, lenghts\n",
    "\n",
    "def get_batch(batch_size, split='train'):\n",
    "    x, y = {\n",
    "        'train': (xtrain, ytrain),\n",
    "        'val': (xval, yval),\n",
    "        'test': (xtest, ytest)\n",
    "    }[split]\n",
    "    # x = xtrain if split == 'train' else xval\n",
    "    # y = ytrain if split == 'train' else yval\n",
    "    idx = torch.randint(0, len(x), (batch_size,))\n",
    "    xb = [encode(x[i]) for i in idx]\n",
    "    yidx = [y[i] for i in idx]\n",
    "    phoneme_sequences = [encode_phonemes(p.split(' ')) for p in yidx]\n",
    "    yb, p_lengths = pad_sequences(phoneme_sequences)\n",
    "    xb, lengths = pad_sequences(xb)\n",
    "    return xb, yb, lengths, p_lengths\n",
    "\n",
    "\n",
    "get_batch(10)\n",
    "\n"
   ],
   "id": "eb3c4d56f78217bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16, 27, 10, 22, 11, 21, 18, 23, 16,  0,  0],\n",
       "         [10, 23, 23, 14, 29, 29,  0,  0,  0,  0,  0],\n",
       "         [27, 14, 18, 12, 17, 17, 24, 21, 13,  1, 28],\n",
       "         [27, 14, 11, 14, 21, 24,  0,  0,  0,  0,  0],\n",
       "         [13, 27, 18, 23, 20, 10, 11, 21, 14,  0,  0],\n",
       "         [32, 18, 12, 14,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [11, 30, 27, 29, 24, 23,  0,  0,  0,  0,  0],\n",
       "         [20, 10, 27, 25, 15,  0,  0,  0,  0,  0,  0],\n",
       "         [22, 10, 27, 20, 18, 14,  0,  0,  0,  0,  0],\n",
       "         [12, 27, 18, 28, 17, 22, 10, 23,  0,  0,  0]]),\n",
       " tensor([[30, 29, 14,  8,  2, 23, 19, 74, 25, 18,  0],\n",
       "         [30, 63, 55, 27, 76, 18,  0,  0,  0,  0,  0],\n",
       "         [30, 14, 20, 59, 65,  7, 19,  5, 70, 18,  0],\n",
       "         [30, 14, 13, 23, 27, 19, 32, 18,  0,  0,  0],\n",
       "         [30,  5, 14, 66, 55, 59, 63, 23, 63, 19, 18],\n",
       "         [30, 72, 20, 52, 18,  0,  0,  0,  0,  0,  0],\n",
       "         [30, 23, 57, 76, 63, 55, 18,  0,  0,  0,  0],\n",
       "         [30, 59, 39, 14, 43, 28, 18,  0,  0,  0,  0],\n",
       "         [30,  2, 39, 14, 59,  1, 18,  0,  0,  0,  0],\n",
       "         [30, 59, 14, 66, 53,  2, 63, 55, 18,  0,  0]]),\n",
       " tensor([ 9,  6, 11,  6,  9,  4,  6,  5,  6,  8]),\n",
       " tensor([10,  6, 10,  8, 11,  5,  7,  7,  7,  9]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.666225Z",
     "start_time": "2025-05-17T09:11:32.653398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ],
   "id": "fdf3e21d36d657f2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:32.670481Z",
     "start_time": "2025-05-17T09:11:32.668708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training with LSTM with multiplicative attention\n",
    "embedding_dim = 32\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "attention_dim = 64\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "learning_rate = 1e-3\n",
    "max_iter = 5000\n",
    "eval_interval = 500\n"
   ],
   "id": "a95d65ea8e4126c4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:11:59.159454Z",
     "start_time": "2025-05-17T09:11:32.677137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1-batch overfitting\n",
    "model = EncoderDecoderLSTMWithMA(vocab_size, hidden_size, phoneme_vocab_size, embedding_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "xb, yb, lengths, p_lengths = get_batch(10)\n",
    "xb, yb, lengths, p_lengths = xb.to(device), yb.to(device), lengths.to(device), p_lengths.to(device)\n",
    "\n",
    "for step in range(1000):\n",
    "    logits = model(xb, lengths, yb, p_lengths)\n",
    "    B, T, V = logits.shape\n",
    "    logits = logits.view(B*T, V)\n",
    "    yb_mod = yb.view(B*T)\n",
    "    loss = F.cross_entropy(logits, yb_mod)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"step: {step}, loss: {loss.item():.4f}\")"
   ],
   "id": "20e39b0405abea8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.3758\n",
      "step: 1, loss: 4.3519\n",
      "step: 2, loss: 4.3278\n",
      "step: 3, loss: 4.3030\n",
      "step: 4, loss: 4.2772\n",
      "step: 5, loss: 4.2501\n",
      "step: 6, loss: 4.2210\n",
      "step: 7, loss: 4.1894\n",
      "step: 8, loss: 4.1545\n",
      "step: 9, loss: 4.1152\n",
      "step: 10, loss: 4.0704\n",
      "step: 11, loss: 4.0183\n",
      "step: 12, loss: 3.9570\n",
      "step: 13, loss: 3.8839\n",
      "step: 14, loss: 3.7969\n",
      "step: 15, loss: 3.6948\n",
      "step: 16, loss: 3.5791\n",
      "step: 17, loss: 3.4555\n",
      "step: 18, loss: 3.3326\n",
      "step: 19, loss: 3.2186\n",
      "step: 20, loss: 3.1187\n",
      "step: 21, loss: 3.0344\n",
      "step: 22, loss: 2.9647\n",
      "step: 23, loss: 2.9072\n",
      "step: 24, loss: 2.8588\n",
      "step: 25, loss: 2.8167\n",
      "step: 26, loss: 2.7781\n",
      "step: 27, loss: 2.7406\n",
      "step: 28, loss: 2.7020\n",
      "step: 29, loss: 2.6609\n",
      "step: 30, loss: 2.6162\n",
      "step: 31, loss: 2.5675\n",
      "step: 32, loss: 2.5163\n",
      "step: 33, loss: 2.4664\n",
      "step: 34, loss: 2.4215\n",
      "step: 35, loss: 2.3831\n",
      "step: 36, loss: 2.3487\n",
      "step: 37, loss: 2.3151\n",
      "step: 38, loss: 2.2801\n",
      "step: 39, loss: 2.2430\n",
      "step: 40, loss: 2.2048\n",
      "step: 41, loss: 2.1669\n",
      "step: 42, loss: 2.1305\n",
      "step: 43, loss: 2.0962\n",
      "step: 44, loss: 2.0633\n",
      "step: 45, loss: 2.0305\n",
      "step: 46, loss: 1.9973\n",
      "step: 47, loss: 1.9643\n",
      "step: 48, loss: 1.9330\n",
      "step: 49, loss: 1.9040\n",
      "step: 50, loss: 1.8750\n",
      "step: 51, loss: 1.8449\n",
      "step: 52, loss: 1.8143\n",
      "step: 53, loss: 1.7848\n",
      "step: 54, loss: 1.7566\n",
      "step: 55, loss: 1.7287\n",
      "step: 56, loss: 1.7009\n",
      "step: 57, loss: 1.6736\n",
      "step: 58, loss: 1.6469\n",
      "step: 59, loss: 1.6201\n",
      "step: 60, loss: 1.5928\n",
      "step: 61, loss: 1.5658\n",
      "step: 62, loss: 1.5397\n",
      "step: 63, loss: 1.5139\n",
      "step: 64, loss: 1.4881\n",
      "step: 65, loss: 1.4626\n",
      "step: 66, loss: 1.4376\n",
      "step: 67, loss: 1.4129\n",
      "step: 68, loss: 1.3883\n",
      "step: 69, loss: 1.3641\n",
      "step: 70, loss: 1.3404\n",
      "step: 71, loss: 1.3168\n",
      "step: 72, loss: 1.2931\n",
      "step: 73, loss: 1.2696\n",
      "step: 74, loss: 1.2462\n",
      "step: 75, loss: 1.2227\n",
      "step: 76, loss: 1.1993\n",
      "step: 77, loss: 1.1763\n",
      "step: 78, loss: 1.1537\n",
      "step: 79, loss: 1.1314\n",
      "step: 80, loss: 1.1096\n",
      "step: 81, loss: 1.0881\n",
      "step: 82, loss: 1.0666\n",
      "step: 83, loss: 1.0454\n",
      "step: 84, loss: 1.0244\n",
      "step: 85, loss: 1.0035\n",
      "step: 86, loss: 0.9828\n",
      "step: 87, loss: 0.9625\n",
      "step: 88, loss: 0.9424\n",
      "step: 89, loss: 0.9226\n",
      "step: 90, loss: 0.9031\n",
      "step: 91, loss: 0.8839\n",
      "step: 92, loss: 0.8649\n",
      "step: 93, loss: 0.8462\n",
      "step: 94, loss: 0.8279\n",
      "step: 95, loss: 0.8098\n",
      "step: 96, loss: 0.7920\n",
      "step: 97, loss: 0.7745\n",
      "step: 98, loss: 0.7573\n",
      "step: 99, loss: 0.7403\n",
      "step: 100, loss: 0.7236\n",
      "step: 101, loss: 0.7071\n",
      "step: 102, loss: 0.6909\n",
      "step: 103, loss: 0.6750\n",
      "step: 104, loss: 0.6594\n",
      "step: 105, loss: 0.6441\n",
      "step: 106, loss: 0.6292\n",
      "step: 107, loss: 0.6146\n",
      "step: 108, loss: 0.6002\n",
      "step: 109, loss: 0.5862\n",
      "step: 110, loss: 0.5724\n",
      "step: 111, loss: 0.5590\n",
      "step: 112, loss: 0.5458\n",
      "step: 113, loss: 0.5328\n",
      "step: 114, loss: 0.5202\n",
      "step: 115, loss: 0.5078\n",
      "step: 116, loss: 0.4958\n",
      "step: 117, loss: 0.4840\n",
      "step: 118, loss: 0.4725\n",
      "step: 119, loss: 0.4612\n",
      "step: 120, loss: 0.4503\n",
      "step: 121, loss: 0.4396\n",
      "step: 122, loss: 0.4291\n",
      "step: 123, loss: 0.4190\n",
      "step: 124, loss: 0.4090\n",
      "step: 125, loss: 0.3994\n",
      "step: 126, loss: 0.3900\n",
      "step: 127, loss: 0.3808\n",
      "step: 128, loss: 0.3719\n",
      "step: 129, loss: 0.3632\n",
      "step: 130, loss: 0.3547\n",
      "step: 131, loss: 0.3464\n",
      "step: 132, loss: 0.3384\n",
      "step: 133, loss: 0.3306\n",
      "step: 134, loss: 0.3230\n",
      "step: 135, loss: 0.3156\n",
      "step: 136, loss: 0.3084\n",
      "step: 137, loss: 0.3014\n",
      "step: 138, loss: 0.2946\n",
      "step: 139, loss: 0.2880\n",
      "step: 140, loss: 0.2815\n",
      "step: 141, loss: 0.2753\n",
      "step: 142, loss: 0.2692\n",
      "step: 143, loss: 0.2633\n",
      "step: 144, loss: 0.2575\n",
      "step: 145, loss: 0.2519\n",
      "step: 146, loss: 0.2465\n",
      "step: 147, loss: 0.2412\n",
      "step: 148, loss: 0.2361\n",
      "step: 149, loss: 0.2311\n",
      "step: 150, loss: 0.2263\n",
      "step: 151, loss: 0.2216\n",
      "step: 152, loss: 0.2170\n",
      "step: 153, loss: 0.2126\n",
      "step: 154, loss: 0.2083\n",
      "step: 155, loss: 0.2041\n",
      "step: 156, loss: 0.2000\n",
      "step: 157, loss: 0.1961\n",
      "step: 158, loss: 0.1922\n",
      "step: 159, loss: 0.1885\n",
      "step: 160, loss: 0.1849\n",
      "step: 161, loss: 0.1813\n",
      "step: 162, loss: 0.1779\n",
      "step: 163, loss: 0.1746\n",
      "step: 164, loss: 0.1713\n",
      "step: 165, loss: 0.1682\n",
      "step: 166, loss: 0.1651\n",
      "step: 167, loss: 0.1621\n",
      "step: 168, loss: 0.1592\n",
      "step: 169, loss: 0.1564\n",
      "step: 170, loss: 0.1537\n",
      "step: 171, loss: 0.1510\n",
      "step: 172, loss: 0.1484\n",
      "step: 173, loss: 0.1459\n",
      "step: 174, loss: 0.1434\n",
      "step: 175, loss: 0.1410\n",
      "step: 176, loss: 0.1386\n",
      "step: 177, loss: 0.1364\n",
      "step: 178, loss: 0.1341\n",
      "step: 179, loss: 0.1320\n",
      "step: 180, loss: 0.1299\n",
      "step: 181, loss: 0.1278\n",
      "step: 182, loss: 0.1258\n",
      "step: 183, loss: 0.1238\n",
      "step: 184, loss: 0.1219\n",
      "step: 185, loss: 0.1200\n",
      "step: 186, loss: 0.1182\n",
      "step: 187, loss: 0.1164\n",
      "step: 188, loss: 0.1147\n",
      "step: 189, loss: 0.1130\n",
      "step: 190, loss: 0.1114\n",
      "step: 191, loss: 0.1097\n",
      "step: 192, loss: 0.1082\n",
      "step: 193, loss: 0.1066\n",
      "step: 194, loss: 0.1051\n",
      "step: 195, loss: 0.1036\n",
      "step: 196, loss: 0.1022\n",
      "step: 197, loss: 0.1008\n",
      "step: 198, loss: 0.0994\n",
      "step: 199, loss: 0.0981\n",
      "step: 200, loss: 0.0967\n",
      "step: 201, loss: 0.0954\n",
      "step: 202, loss: 0.0942\n",
      "step: 203, loss: 0.0929\n",
      "step: 204, loss: 0.0917\n",
      "step: 205, loss: 0.0906\n",
      "step: 206, loss: 0.0894\n",
      "step: 207, loss: 0.0883\n",
      "step: 208, loss: 0.0871\n",
      "step: 209, loss: 0.0861\n",
      "step: 210, loss: 0.0850\n",
      "step: 211, loss: 0.0839\n",
      "step: 212, loss: 0.0829\n",
      "step: 213, loss: 0.0819\n",
      "step: 214, loss: 0.0809\n",
      "step: 215, loss: 0.0800\n",
      "step: 216, loss: 0.0790\n",
      "step: 217, loss: 0.0781\n",
      "step: 218, loss: 0.0772\n",
      "step: 219, loss: 0.0763\n",
      "step: 220, loss: 0.0754\n",
      "step: 221, loss: 0.0745\n",
      "step: 222, loss: 0.0737\n",
      "step: 223, loss: 0.0729\n",
      "step: 224, loss: 0.0721\n",
      "step: 225, loss: 0.0713\n",
      "step: 226, loss: 0.0705\n",
      "step: 227, loss: 0.0697\n",
      "step: 228, loss: 0.0689\n",
      "step: 229, loss: 0.0682\n",
      "step: 230, loss: 0.0675\n",
      "step: 231, loss: 0.0668\n",
      "step: 232, loss: 0.0660\n",
      "step: 233, loss: 0.0654\n",
      "step: 234, loss: 0.0647\n",
      "step: 235, loss: 0.0640\n",
      "step: 236, loss: 0.0633\n",
      "step: 237, loss: 0.0627\n",
      "step: 238, loss: 0.0621\n",
      "step: 239, loss: 0.0614\n",
      "step: 240, loss: 0.0608\n",
      "step: 241, loss: 0.0602\n",
      "step: 242, loss: 0.0596\n",
      "step: 243, loss: 0.0590\n",
      "step: 244, loss: 0.0585\n",
      "step: 245, loss: 0.0579\n",
      "step: 246, loss: 0.0573\n",
      "step: 247, loss: 0.0568\n",
      "step: 248, loss: 0.0562\n",
      "step: 249, loss: 0.0557\n",
      "step: 250, loss: 0.0552\n",
      "step: 251, loss: 0.0547\n",
      "step: 252, loss: 0.0542\n",
      "step: 253, loss: 0.0537\n",
      "step: 254, loss: 0.0532\n",
      "step: 255, loss: 0.0527\n",
      "step: 256, loss: 0.0522\n",
      "step: 257, loss: 0.0517\n",
      "step: 258, loss: 0.0513\n",
      "step: 259, loss: 0.0508\n",
      "step: 260, loss: 0.0504\n",
      "step: 261, loss: 0.0499\n",
      "step: 262, loss: 0.0495\n",
      "step: 263, loss: 0.0491\n",
      "step: 264, loss: 0.0486\n",
      "step: 265, loss: 0.0482\n",
      "step: 266, loss: 0.0478\n",
      "step: 267, loss: 0.0474\n",
      "step: 268, loss: 0.0470\n",
      "step: 269, loss: 0.0466\n",
      "step: 270, loss: 0.0462\n",
      "step: 271, loss: 0.0458\n",
      "step: 272, loss: 0.0454\n",
      "step: 273, loss: 0.0451\n",
      "step: 274, loss: 0.0447\n",
      "step: 275, loss: 0.0443\n",
      "step: 276, loss: 0.0440\n",
      "step: 277, loss: 0.0436\n",
      "step: 278, loss: 0.0433\n",
      "step: 279, loss: 0.0429\n",
      "step: 280, loss: 0.0426\n",
      "step: 281, loss: 0.0422\n",
      "step: 282, loss: 0.0419\n",
      "step: 283, loss: 0.0416\n",
      "step: 284, loss: 0.0412\n",
      "step: 285, loss: 0.0409\n",
      "step: 286, loss: 0.0406\n",
      "step: 287, loss: 0.0403\n",
      "step: 288, loss: 0.0400\n",
      "step: 289, loss: 0.0397\n",
      "step: 290, loss: 0.0394\n",
      "step: 291, loss: 0.0391\n",
      "step: 292, loss: 0.0388\n",
      "step: 293, loss: 0.0385\n",
      "step: 294, loss: 0.0382\n",
      "step: 295, loss: 0.0379\n",
      "step: 296, loss: 0.0376\n",
      "step: 297, loss: 0.0374\n",
      "step: 298, loss: 0.0371\n",
      "step: 299, loss: 0.0368\n",
      "step: 300, loss: 0.0366\n",
      "step: 301, loss: 0.0363\n",
      "step: 302, loss: 0.0360\n",
      "step: 303, loss: 0.0358\n",
      "step: 304, loss: 0.0355\n",
      "step: 305, loss: 0.0353\n",
      "step: 306, loss: 0.0350\n",
      "step: 307, loss: 0.0348\n",
      "step: 308, loss: 0.0345\n",
      "step: 309, loss: 0.0343\n",
      "step: 310, loss: 0.0341\n",
      "step: 311, loss: 0.0338\n",
      "step: 312, loss: 0.0336\n",
      "step: 313, loss: 0.0334\n",
      "step: 314, loss: 0.0331\n",
      "step: 315, loss: 0.0329\n",
      "step: 316, loss: 0.0327\n",
      "step: 317, loss: 0.0325\n",
      "step: 318, loss: 0.0322\n",
      "step: 319, loss: 0.0320\n",
      "step: 320, loss: 0.0318\n",
      "step: 321, loss: 0.0316\n",
      "step: 322, loss: 0.0314\n",
      "step: 323, loss: 0.0312\n",
      "step: 324, loss: 0.0310\n",
      "step: 325, loss: 0.0308\n",
      "step: 326, loss: 0.0306\n",
      "step: 327, loss: 0.0304\n",
      "step: 328, loss: 0.0302\n",
      "step: 329, loss: 0.0300\n",
      "step: 330, loss: 0.0298\n",
      "step: 331, loss: 0.0296\n",
      "step: 332, loss: 0.0294\n",
      "step: 333, loss: 0.0292\n",
      "step: 334, loss: 0.0291\n",
      "step: 335, loss: 0.0289\n",
      "step: 336, loss: 0.0287\n",
      "step: 337, loss: 0.0285\n",
      "step: 338, loss: 0.0283\n",
      "step: 339, loss: 0.0282\n",
      "step: 340, loss: 0.0280\n",
      "step: 341, loss: 0.0278\n",
      "step: 342, loss: 0.0277\n",
      "step: 343, loss: 0.0275\n",
      "step: 344, loss: 0.0273\n",
      "step: 345, loss: 0.0272\n",
      "step: 346, loss: 0.0270\n",
      "step: 347, loss: 0.0268\n",
      "step: 348, loss: 0.0267\n",
      "step: 349, loss: 0.0265\n",
      "step: 350, loss: 0.0264\n",
      "step: 351, loss: 0.0262\n",
      "step: 352, loss: 0.0261\n",
      "step: 353, loss: 0.0259\n",
      "step: 354, loss: 0.0257\n",
      "step: 355, loss: 0.0256\n",
      "step: 356, loss: 0.0255\n",
      "step: 357, loss: 0.0253\n",
      "step: 358, loss: 0.0252\n",
      "step: 359, loss: 0.0250\n",
      "step: 360, loss: 0.0249\n",
      "step: 361, loss: 0.0247\n",
      "step: 362, loss: 0.0246\n",
      "step: 363, loss: 0.0245\n",
      "step: 364, loss: 0.0243\n",
      "step: 365, loss: 0.0242\n",
      "step: 366, loss: 0.0240\n",
      "step: 367, loss: 0.0239\n",
      "step: 368, loss: 0.0238\n",
      "step: 369, loss: 0.0237\n",
      "step: 370, loss: 0.0235\n",
      "step: 371, loss: 0.0234\n",
      "step: 372, loss: 0.0233\n",
      "step: 373, loss: 0.0231\n",
      "step: 374, loss: 0.0230\n",
      "step: 375, loss: 0.0229\n",
      "step: 376, loss: 0.0228\n",
      "step: 377, loss: 0.0226\n",
      "step: 378, loss: 0.0225\n",
      "step: 379, loss: 0.0224\n",
      "step: 380, loss: 0.0223\n",
      "step: 381, loss: 0.0222\n",
      "step: 382, loss: 0.0220\n",
      "step: 383, loss: 0.0219\n",
      "step: 384, loss: 0.0218\n",
      "step: 385, loss: 0.0217\n",
      "step: 386, loss: 0.0216\n",
      "step: 387, loss: 0.0215\n",
      "step: 388, loss: 0.0214\n",
      "step: 389, loss: 0.0213\n",
      "step: 390, loss: 0.0211\n",
      "step: 391, loss: 0.0210\n",
      "step: 392, loss: 0.0209\n",
      "step: 393, loss: 0.0208\n",
      "step: 394, loss: 0.0207\n",
      "step: 395, loss: 0.0206\n",
      "step: 396, loss: 0.0205\n",
      "step: 397, loss: 0.0204\n",
      "step: 398, loss: 0.0203\n",
      "step: 399, loss: 0.0202\n",
      "step: 400, loss: 0.0201\n",
      "step: 401, loss: 0.0200\n",
      "step: 402, loss: 0.0199\n",
      "step: 403, loss: 0.0198\n",
      "step: 404, loss: 0.0197\n",
      "step: 405, loss: 0.0196\n",
      "step: 406, loss: 0.0195\n",
      "step: 407, loss: 0.0194\n",
      "step: 408, loss: 0.0193\n",
      "step: 409, loss: 0.0192\n",
      "step: 410, loss: 0.0191\n",
      "step: 411, loss: 0.0190\n",
      "step: 412, loss: 0.0190\n",
      "step: 413, loss: 0.0189\n",
      "step: 414, loss: 0.0188\n",
      "step: 415, loss: 0.0187\n",
      "step: 416, loss: 0.0186\n",
      "step: 417, loss: 0.0185\n",
      "step: 418, loss: 0.0184\n",
      "step: 419, loss: 0.0183\n",
      "step: 420, loss: 0.0183\n",
      "step: 421, loss: 0.0182\n",
      "step: 422, loss: 0.0181\n",
      "step: 423, loss: 0.0180\n",
      "step: 424, loss: 0.0179\n",
      "step: 425, loss: 0.0178\n",
      "step: 426, loss: 0.0177\n",
      "step: 427, loss: 0.0177\n",
      "step: 428, loss: 0.0176\n",
      "step: 429, loss: 0.0175\n",
      "step: 430, loss: 0.0174\n",
      "step: 431, loss: 0.0173\n",
      "step: 432, loss: 0.0173\n",
      "step: 433, loss: 0.0172\n",
      "step: 434, loss: 0.0171\n",
      "step: 435, loss: 0.0170\n",
      "step: 436, loss: 0.0170\n",
      "step: 437, loss: 0.0169\n",
      "step: 438, loss: 0.0168\n",
      "step: 439, loss: 0.0167\n",
      "step: 440, loss: 0.0167\n",
      "step: 441, loss: 0.0166\n",
      "step: 442, loss: 0.0165\n",
      "step: 443, loss: 0.0164\n",
      "step: 444, loss: 0.0164\n",
      "step: 445, loss: 0.0163\n",
      "step: 446, loss: 0.0162\n",
      "step: 447, loss: 0.0162\n",
      "step: 448, loss: 0.0161\n",
      "step: 449, loss: 0.0160\n",
      "step: 450, loss: 0.0159\n",
      "step: 451, loss: 0.0159\n",
      "step: 452, loss: 0.0158\n",
      "step: 453, loss: 0.0157\n",
      "step: 454, loss: 0.0157\n",
      "step: 455, loss: 0.0156\n",
      "step: 456, loss: 0.0155\n",
      "step: 457, loss: 0.0155\n",
      "step: 458, loss: 0.0154\n",
      "step: 459, loss: 0.0153\n",
      "step: 460, loss: 0.0153\n",
      "step: 461, loss: 0.0152\n",
      "step: 462, loss: 0.0152\n",
      "step: 463, loss: 0.0151\n",
      "step: 464, loss: 0.0150\n",
      "step: 465, loss: 0.0150\n",
      "step: 466, loss: 0.0149\n",
      "step: 467, loss: 0.0148\n",
      "step: 468, loss: 0.0148\n",
      "step: 469, loss: 0.0147\n",
      "step: 470, loss: 0.0147\n",
      "step: 471, loss: 0.0146\n",
      "step: 472, loss: 0.0145\n",
      "step: 473, loss: 0.0145\n",
      "step: 474, loss: 0.0144\n",
      "step: 475, loss: 0.0144\n",
      "step: 476, loss: 0.0143\n",
      "step: 477, loss: 0.0142\n",
      "step: 478, loss: 0.0142\n",
      "step: 479, loss: 0.0141\n",
      "step: 480, loss: 0.0141\n",
      "step: 481, loss: 0.0140\n",
      "step: 482, loss: 0.0140\n",
      "step: 483, loss: 0.0139\n",
      "step: 484, loss: 0.0139\n",
      "step: 485, loss: 0.0138\n",
      "step: 486, loss: 0.0137\n",
      "step: 487, loss: 0.0137\n",
      "step: 488, loss: 0.0136\n",
      "step: 489, loss: 0.0136\n",
      "step: 490, loss: 0.0135\n",
      "step: 491, loss: 0.0135\n",
      "step: 492, loss: 0.0134\n",
      "step: 493, loss: 0.0134\n",
      "step: 494, loss: 0.0133\n",
      "step: 495, loss: 0.0133\n",
      "step: 496, loss: 0.0132\n",
      "step: 497, loss: 0.0132\n",
      "step: 498, loss: 0.0131\n",
      "step: 499, loss: 0.0131\n",
      "step: 500, loss: 0.0130\n",
      "step: 501, loss: 0.0130\n",
      "step: 502, loss: 0.0129\n",
      "step: 503, loss: 0.0129\n",
      "step: 504, loss: 0.0128\n",
      "step: 505, loss: 0.0128\n",
      "step: 506, loss: 0.0127\n",
      "step: 507, loss: 0.0127\n",
      "step: 508, loss: 0.0126\n",
      "step: 509, loss: 0.0126\n",
      "step: 510, loss: 0.0125\n",
      "step: 511, loss: 0.0125\n",
      "step: 512, loss: 0.0124\n",
      "step: 513, loss: 0.0124\n",
      "step: 514, loss: 0.0123\n",
      "step: 515, loss: 0.0123\n",
      "step: 516, loss: 0.0123\n",
      "step: 517, loss: 0.0122\n",
      "step: 518, loss: 0.0122\n",
      "step: 519, loss: 0.0121\n",
      "step: 520, loss: 0.0121\n",
      "step: 521, loss: 0.0120\n",
      "step: 522, loss: 0.0120\n",
      "step: 523, loss: 0.0119\n",
      "step: 524, loss: 0.0119\n",
      "step: 525, loss: 0.0119\n",
      "step: 526, loss: 0.0118\n",
      "step: 527, loss: 0.0118\n",
      "step: 528, loss: 0.0117\n",
      "step: 529, loss: 0.0117\n",
      "step: 530, loss: 0.0117\n",
      "step: 531, loss: 0.0116\n",
      "step: 532, loss: 0.0116\n",
      "step: 533, loss: 0.0115\n",
      "step: 534, loss: 0.0115\n",
      "step: 535, loss: 0.0114\n",
      "step: 536, loss: 0.0114\n",
      "step: 537, loss: 0.0114\n",
      "step: 538, loss: 0.0113\n",
      "step: 539, loss: 0.0113\n",
      "step: 540, loss: 0.0112\n",
      "step: 541, loss: 0.0112\n",
      "step: 542, loss: 0.0112\n",
      "step: 543, loss: 0.0111\n",
      "step: 544, loss: 0.0111\n",
      "step: 545, loss: 0.0111\n",
      "step: 546, loss: 0.0110\n",
      "step: 547, loss: 0.0110\n",
      "step: 548, loss: 0.0109\n",
      "step: 549, loss: 0.0109\n",
      "step: 550, loss: 0.0109\n",
      "step: 551, loss: 0.0108\n",
      "step: 552, loss: 0.0108\n",
      "step: 553, loss: 0.0108\n",
      "step: 554, loss: 0.0107\n",
      "step: 555, loss: 0.0107\n",
      "step: 556, loss: 0.0106\n",
      "step: 557, loss: 0.0106\n",
      "step: 558, loss: 0.0106\n",
      "step: 559, loss: 0.0105\n",
      "step: 560, loss: 0.0105\n",
      "step: 561, loss: 0.0105\n",
      "step: 562, loss: 0.0104\n",
      "step: 563, loss: 0.0104\n",
      "step: 564, loss: 0.0104\n",
      "step: 565, loss: 0.0103\n",
      "step: 566, loss: 0.0103\n",
      "step: 567, loss: 0.0103\n",
      "step: 568, loss: 0.0102\n",
      "step: 569, loss: 0.0102\n",
      "step: 570, loss: 0.0102\n",
      "step: 571, loss: 0.0101\n",
      "step: 572, loss: 0.0101\n",
      "step: 573, loss: 0.0101\n",
      "step: 574, loss: 0.0100\n",
      "step: 575, loss: 0.0100\n",
      "step: 576, loss: 0.0100\n",
      "step: 577, loss: 0.0099\n",
      "step: 578, loss: 0.0099\n",
      "step: 579, loss: 0.0099\n",
      "step: 580, loss: 0.0098\n",
      "step: 581, loss: 0.0098\n",
      "step: 582, loss: 0.0098\n",
      "step: 583, loss: 0.0097\n",
      "step: 584, loss: 0.0097\n",
      "step: 585, loss: 0.0097\n",
      "step: 586, loss: 0.0097\n",
      "step: 587, loss: 0.0096\n",
      "step: 588, loss: 0.0096\n",
      "step: 589, loss: 0.0096\n",
      "step: 590, loss: 0.0095\n",
      "step: 591, loss: 0.0095\n",
      "step: 592, loss: 0.0095\n",
      "step: 593, loss: 0.0094\n",
      "step: 594, loss: 0.0094\n",
      "step: 595, loss: 0.0094\n",
      "step: 596, loss: 0.0094\n",
      "step: 597, loss: 0.0093\n",
      "step: 598, loss: 0.0093\n",
      "step: 599, loss: 0.0093\n",
      "step: 600, loss: 0.0092\n",
      "step: 601, loss: 0.0092\n",
      "step: 602, loss: 0.0092\n",
      "step: 603, loss: 0.0092\n",
      "step: 604, loss: 0.0091\n",
      "step: 605, loss: 0.0091\n",
      "step: 606, loss: 0.0091\n",
      "step: 607, loss: 0.0090\n",
      "step: 608, loss: 0.0090\n",
      "step: 609, loss: 0.0090\n",
      "step: 610, loss: 0.0090\n",
      "step: 611, loss: 0.0089\n",
      "step: 612, loss: 0.0089\n",
      "step: 613, loss: 0.0089\n",
      "step: 614, loss: 0.0089\n",
      "step: 615, loss: 0.0088\n",
      "step: 616, loss: 0.0088\n",
      "step: 617, loss: 0.0088\n",
      "step: 618, loss: 0.0087\n",
      "step: 619, loss: 0.0087\n",
      "step: 620, loss: 0.0087\n",
      "step: 621, loss: 0.0087\n",
      "step: 622, loss: 0.0086\n",
      "step: 623, loss: 0.0086\n",
      "step: 624, loss: 0.0086\n",
      "step: 625, loss: 0.0086\n",
      "step: 626, loss: 0.0085\n",
      "step: 627, loss: 0.0085\n",
      "step: 628, loss: 0.0085\n",
      "step: 629, loss: 0.0085\n",
      "step: 630, loss: 0.0084\n",
      "step: 631, loss: 0.0084\n",
      "step: 632, loss: 0.0084\n",
      "step: 633, loss: 0.0084\n",
      "step: 634, loss: 0.0083\n",
      "step: 635, loss: 0.0083\n",
      "step: 636, loss: 0.0083\n",
      "step: 637, loss: 0.0083\n",
      "step: 638, loss: 0.0082\n",
      "step: 639, loss: 0.0082\n",
      "step: 640, loss: 0.0082\n",
      "step: 641, loss: 0.0082\n",
      "step: 642, loss: 0.0082\n",
      "step: 643, loss: 0.0081\n",
      "step: 644, loss: 0.0081\n",
      "step: 645, loss: 0.0081\n",
      "step: 646, loss: 0.0081\n",
      "step: 647, loss: 0.0080\n",
      "step: 648, loss: 0.0080\n",
      "step: 649, loss: 0.0080\n",
      "step: 650, loss: 0.0080\n",
      "step: 651, loss: 0.0079\n",
      "step: 652, loss: 0.0079\n",
      "step: 653, loss: 0.0079\n",
      "step: 654, loss: 0.0079\n",
      "step: 655, loss: 0.0079\n",
      "step: 656, loss: 0.0078\n",
      "step: 657, loss: 0.0078\n",
      "step: 658, loss: 0.0078\n",
      "step: 659, loss: 0.0078\n",
      "step: 660, loss: 0.0077\n",
      "step: 661, loss: 0.0077\n",
      "step: 662, loss: 0.0077\n",
      "step: 663, loss: 0.0077\n",
      "step: 664, loss: 0.0077\n",
      "step: 665, loss: 0.0076\n",
      "step: 666, loss: 0.0076\n",
      "step: 667, loss: 0.0076\n",
      "step: 668, loss: 0.0076\n",
      "step: 669, loss: 0.0076\n",
      "step: 670, loss: 0.0075\n",
      "step: 671, loss: 0.0075\n",
      "step: 672, loss: 0.0075\n",
      "step: 673, loss: 0.0075\n",
      "step: 674, loss: 0.0075\n",
      "step: 675, loss: 0.0074\n",
      "step: 676, loss: 0.0074\n",
      "step: 677, loss: 0.0074\n",
      "step: 678, loss: 0.0074\n",
      "step: 679, loss: 0.0074\n",
      "step: 680, loss: 0.0073\n",
      "step: 681, loss: 0.0073\n",
      "step: 682, loss: 0.0073\n",
      "step: 683, loss: 0.0073\n",
      "step: 684, loss: 0.0073\n",
      "step: 685, loss: 0.0072\n",
      "step: 686, loss: 0.0072\n",
      "step: 687, loss: 0.0072\n",
      "step: 688, loss: 0.0072\n",
      "step: 689, loss: 0.0072\n",
      "step: 690, loss: 0.0071\n",
      "step: 691, loss: 0.0071\n",
      "step: 692, loss: 0.0071\n",
      "step: 693, loss: 0.0071\n",
      "step: 694, loss: 0.0071\n",
      "step: 695, loss: 0.0071\n",
      "step: 696, loss: 0.0070\n",
      "step: 697, loss: 0.0070\n",
      "step: 698, loss: 0.0070\n",
      "step: 699, loss: 0.0070\n",
      "step: 700, loss: 0.0070\n",
      "step: 701, loss: 0.0069\n",
      "step: 702, loss: 0.0069\n",
      "step: 703, loss: 0.0069\n",
      "step: 704, loss: 0.0069\n",
      "step: 705, loss: 0.0069\n",
      "step: 706, loss: 0.0069\n",
      "step: 707, loss: 0.0068\n",
      "step: 708, loss: 0.0068\n",
      "step: 709, loss: 0.0068\n",
      "step: 710, loss: 0.0068\n",
      "step: 711, loss: 0.0068\n",
      "step: 712, loss: 0.0067\n",
      "step: 713, loss: 0.0067\n",
      "step: 714, loss: 0.0067\n",
      "step: 715, loss: 0.0067\n",
      "step: 716, loss: 0.0067\n",
      "step: 717, loss: 0.0067\n",
      "step: 718, loss: 0.0066\n",
      "step: 719, loss: 0.0066\n",
      "step: 720, loss: 0.0066\n",
      "step: 721, loss: 0.0066\n",
      "step: 722, loss: 0.0066\n",
      "step: 723, loss: 0.0066\n",
      "step: 724, loss: 0.0065\n",
      "step: 725, loss: 0.0065\n",
      "step: 726, loss: 0.0065\n",
      "step: 727, loss: 0.0065\n",
      "step: 728, loss: 0.0065\n",
      "step: 729, loss: 0.0065\n",
      "step: 730, loss: 0.0064\n",
      "step: 731, loss: 0.0064\n",
      "step: 732, loss: 0.0064\n",
      "step: 733, loss: 0.0064\n",
      "step: 734, loss: 0.0064\n",
      "step: 735, loss: 0.0064\n",
      "step: 736, loss: 0.0064\n",
      "step: 737, loss: 0.0063\n",
      "step: 738, loss: 0.0063\n",
      "step: 739, loss: 0.0063\n",
      "step: 740, loss: 0.0063\n",
      "step: 741, loss: 0.0063\n",
      "step: 742, loss: 0.0063\n",
      "step: 743, loss: 0.0062\n",
      "step: 744, loss: 0.0062\n",
      "step: 745, loss: 0.0062\n",
      "step: 746, loss: 0.0062\n",
      "step: 747, loss: 0.0062\n",
      "step: 748, loss: 0.0062\n",
      "step: 749, loss: 0.0062\n",
      "step: 750, loss: 0.0061\n",
      "step: 751, loss: 0.0061\n",
      "step: 752, loss: 0.0061\n",
      "step: 753, loss: 0.0061\n",
      "step: 754, loss: 0.0061\n",
      "step: 755, loss: 0.0061\n",
      "step: 756, loss: 0.0060\n",
      "step: 757, loss: 0.0060\n",
      "step: 758, loss: 0.0060\n",
      "step: 759, loss: 0.0060\n",
      "step: 760, loss: 0.0060\n",
      "step: 761, loss: 0.0060\n",
      "step: 762, loss: 0.0060\n",
      "step: 763, loss: 0.0059\n",
      "step: 764, loss: 0.0059\n",
      "step: 765, loss: 0.0059\n",
      "step: 766, loss: 0.0059\n",
      "step: 767, loss: 0.0059\n",
      "step: 768, loss: 0.0059\n",
      "step: 769, loss: 0.0059\n",
      "step: 770, loss: 0.0059\n",
      "step: 771, loss: 0.0058\n",
      "step: 772, loss: 0.0058\n",
      "step: 773, loss: 0.0058\n",
      "step: 774, loss: 0.0058\n",
      "step: 775, loss: 0.0058\n",
      "step: 776, loss: 0.0058\n",
      "step: 777, loss: 0.0058\n",
      "step: 778, loss: 0.0057\n",
      "step: 779, loss: 0.0057\n",
      "step: 780, loss: 0.0057\n",
      "step: 781, loss: 0.0057\n",
      "step: 782, loss: 0.0057\n",
      "step: 783, loss: 0.0057\n",
      "step: 784, loss: 0.0057\n",
      "step: 785, loss: 0.0056\n",
      "step: 786, loss: 0.0056\n",
      "step: 787, loss: 0.0056\n",
      "step: 788, loss: 0.0056\n",
      "step: 789, loss: 0.0056\n",
      "step: 790, loss: 0.0056\n",
      "step: 791, loss: 0.0056\n",
      "step: 792, loss: 0.0056\n",
      "step: 793, loss: 0.0055\n",
      "step: 794, loss: 0.0055\n",
      "step: 795, loss: 0.0055\n",
      "step: 796, loss: 0.0055\n",
      "step: 797, loss: 0.0055\n",
      "step: 798, loss: 0.0055\n",
      "step: 799, loss: 0.0055\n",
      "step: 800, loss: 0.0055\n",
      "step: 801, loss: 0.0054\n",
      "step: 802, loss: 0.0054\n",
      "step: 803, loss: 0.0054\n",
      "step: 804, loss: 0.0054\n",
      "step: 805, loss: 0.0054\n",
      "step: 806, loss: 0.0054\n",
      "step: 807, loss: 0.0054\n",
      "step: 808, loss: 0.0054\n",
      "step: 809, loss: 0.0053\n",
      "step: 810, loss: 0.0053\n",
      "step: 811, loss: 0.0053\n",
      "step: 812, loss: 0.0053\n",
      "step: 813, loss: 0.0053\n",
      "step: 814, loss: 0.0053\n",
      "step: 815, loss: 0.0053\n",
      "step: 816, loss: 0.0053\n",
      "step: 817, loss: 0.0053\n",
      "step: 818, loss: 0.0052\n",
      "step: 819, loss: 0.0052\n",
      "step: 820, loss: 0.0052\n",
      "step: 821, loss: 0.0052\n",
      "step: 822, loss: 0.0052\n",
      "step: 823, loss: 0.0052\n",
      "step: 824, loss: 0.0052\n",
      "step: 825, loss: 0.0052\n",
      "step: 826, loss: 0.0052\n",
      "step: 827, loss: 0.0051\n",
      "step: 828, loss: 0.0051\n",
      "step: 829, loss: 0.0051\n",
      "step: 830, loss: 0.0051\n",
      "step: 831, loss: 0.0051\n",
      "step: 832, loss: 0.0051\n",
      "step: 833, loss: 0.0051\n",
      "step: 834, loss: 0.0051\n",
      "step: 835, loss: 0.0051\n",
      "step: 836, loss: 0.0050\n",
      "step: 837, loss: 0.0050\n",
      "step: 838, loss: 0.0050\n",
      "step: 839, loss: 0.0050\n",
      "step: 840, loss: 0.0050\n",
      "step: 841, loss: 0.0050\n",
      "step: 842, loss: 0.0050\n",
      "step: 843, loss: 0.0050\n",
      "step: 844, loss: 0.0050\n",
      "step: 845, loss: 0.0049\n",
      "step: 846, loss: 0.0049\n",
      "step: 847, loss: 0.0049\n",
      "step: 848, loss: 0.0049\n",
      "step: 849, loss: 0.0049\n",
      "step: 850, loss: 0.0049\n",
      "step: 851, loss: 0.0049\n",
      "step: 852, loss: 0.0049\n",
      "step: 853, loss: 0.0049\n",
      "step: 854, loss: 0.0049\n",
      "step: 855, loss: 0.0048\n",
      "step: 856, loss: 0.0048\n",
      "step: 857, loss: 0.0048\n",
      "step: 858, loss: 0.0048\n",
      "step: 859, loss: 0.0048\n",
      "step: 860, loss: 0.0048\n",
      "step: 861, loss: 0.0048\n",
      "step: 862, loss: 0.0048\n",
      "step: 863, loss: 0.0048\n",
      "step: 864, loss: 0.0048\n",
      "step: 865, loss: 0.0047\n",
      "step: 866, loss: 0.0047\n",
      "step: 867, loss: 0.0047\n",
      "step: 868, loss: 0.0047\n",
      "step: 869, loss: 0.0047\n",
      "step: 870, loss: 0.0047\n",
      "step: 871, loss: 0.0047\n",
      "step: 872, loss: 0.0047\n",
      "step: 873, loss: 0.0047\n",
      "step: 874, loss: 0.0047\n",
      "step: 875, loss: 0.0046\n",
      "step: 876, loss: 0.0046\n",
      "step: 877, loss: 0.0046\n",
      "step: 878, loss: 0.0046\n",
      "step: 879, loss: 0.0046\n",
      "step: 880, loss: 0.0046\n",
      "step: 881, loss: 0.0046\n",
      "step: 882, loss: 0.0046\n",
      "step: 883, loss: 0.0046\n",
      "step: 884, loss: 0.0046\n",
      "step: 885, loss: 0.0045\n",
      "step: 886, loss: 0.0045\n",
      "step: 887, loss: 0.0045\n",
      "step: 888, loss: 0.0045\n",
      "step: 889, loss: 0.0045\n",
      "step: 890, loss: 0.0045\n",
      "step: 891, loss: 0.0045\n",
      "step: 892, loss: 0.0045\n",
      "step: 893, loss: 0.0045\n",
      "step: 894, loss: 0.0045\n",
      "step: 895, loss: 0.0045\n",
      "step: 896, loss: 0.0044\n",
      "step: 897, loss: 0.0044\n",
      "step: 898, loss: 0.0044\n",
      "step: 899, loss: 0.0044\n",
      "step: 900, loss: 0.0044\n",
      "step: 901, loss: 0.0044\n",
      "step: 902, loss: 0.0044\n",
      "step: 903, loss: 0.0044\n",
      "step: 904, loss: 0.0044\n",
      "step: 905, loss: 0.0044\n",
      "step: 906, loss: 0.0044\n",
      "step: 907, loss: 0.0044\n",
      "step: 908, loss: 0.0043\n",
      "step: 909, loss: 0.0043\n",
      "step: 910, loss: 0.0043\n",
      "step: 911, loss: 0.0043\n",
      "step: 912, loss: 0.0043\n",
      "step: 913, loss: 0.0043\n",
      "step: 914, loss: 0.0043\n",
      "step: 915, loss: 0.0043\n",
      "step: 916, loss: 0.0043\n",
      "step: 917, loss: 0.0043\n",
      "step: 918, loss: 0.0043\n",
      "step: 919, loss: 0.0043\n",
      "step: 920, loss: 0.0042\n",
      "step: 921, loss: 0.0042\n",
      "step: 922, loss: 0.0042\n",
      "step: 923, loss: 0.0042\n",
      "step: 924, loss: 0.0042\n",
      "step: 925, loss: 0.0042\n",
      "step: 926, loss: 0.0042\n",
      "step: 927, loss: 0.0042\n",
      "step: 928, loss: 0.0042\n",
      "step: 929, loss: 0.0042\n",
      "step: 930, loss: 0.0042\n",
      "step: 931, loss: 0.0042\n",
      "step: 932, loss: 0.0041\n",
      "step: 933, loss: 0.0041\n",
      "step: 934, loss: 0.0041\n",
      "step: 935, loss: 0.0041\n",
      "step: 936, loss: 0.0041\n",
      "step: 937, loss: 0.0041\n",
      "step: 938, loss: 0.0041\n",
      "step: 939, loss: 0.0041\n",
      "step: 940, loss: 0.0041\n",
      "step: 941, loss: 0.0041\n",
      "step: 942, loss: 0.0041\n",
      "step: 943, loss: 0.0041\n",
      "step: 944, loss: 0.0041\n",
      "step: 945, loss: 0.0040\n",
      "step: 946, loss: 0.0040\n",
      "step: 947, loss: 0.0040\n",
      "step: 948, loss: 0.0040\n",
      "step: 949, loss: 0.0040\n",
      "step: 950, loss: 0.0040\n",
      "step: 951, loss: 0.0040\n",
      "step: 952, loss: 0.0040\n",
      "step: 953, loss: 0.0040\n",
      "step: 954, loss: 0.0040\n",
      "step: 955, loss: 0.0040\n",
      "step: 956, loss: 0.0040\n",
      "step: 957, loss: 0.0040\n",
      "step: 958, loss: 0.0039\n",
      "step: 959, loss: 0.0039\n",
      "step: 960, loss: 0.0039\n",
      "step: 961, loss: 0.0039\n",
      "step: 962, loss: 0.0039\n",
      "step: 963, loss: 0.0039\n",
      "step: 964, loss: 0.0039\n",
      "step: 965, loss: 0.0039\n",
      "step: 966, loss: 0.0039\n",
      "step: 967, loss: 0.0039\n",
      "step: 968, loss: 0.0039\n",
      "step: 969, loss: 0.0039\n",
      "step: 970, loss: 0.0039\n",
      "step: 971, loss: 0.0039\n",
      "step: 972, loss: 0.0038\n",
      "step: 973, loss: 0.0038\n",
      "step: 974, loss: 0.0038\n",
      "step: 975, loss: 0.0038\n",
      "step: 976, loss: 0.0038\n",
      "step: 977, loss: 0.0038\n",
      "step: 978, loss: 0.0038\n",
      "step: 979, loss: 0.0038\n",
      "step: 980, loss: 0.0038\n",
      "step: 981, loss: 0.0038\n",
      "step: 982, loss: 0.0038\n",
      "step: 983, loss: 0.0038\n",
      "step: 984, loss: 0.0038\n",
      "step: 985, loss: 0.0038\n",
      "step: 986, loss: 0.0037\n",
      "step: 987, loss: 0.0037\n",
      "step: 988, loss: 0.0037\n",
      "step: 989, loss: 0.0037\n",
      "step: 990, loss: 0.0037\n",
      "step: 991, loss: 0.0037\n",
      "step: 992, loss: 0.0037\n",
      "step: 993, loss: 0.0037\n",
      "step: 994, loss: 0.0037\n",
      "step: 995, loss: 0.0037\n",
      "step: 996, loss: 0.0037\n",
      "step: 997, loss: 0.0037\n",
      "step: 998, loss: 0.0037\n",
      "step: 999, loss: 0.0037\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:17:55.102717Z",
     "start_time": "2025-05-17T09:11:59.292947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# training loop\n",
    "model = EncoderDecoderLSTMWithMA(vocab_size, hidden_size, phoneme_vocab_size, embedding_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iter):\n",
    "    xb, yb, lengths, p_lengths = get_batch(batch_size)\n",
    "    xb, yb, lengths, p_lengths = xb.to(device), yb.to(device), lengths.to(device), p_lengths.to(device)\n",
    "\n",
    "    logits = model(xb, lengths, yb, p_lengths)\n",
    "\n",
    "    B, T, V = logits.shape\n",
    "    logits = logits.view(B*T, V)\n",
    "    yb_mod = yb.view(B*T)\n",
    "    loss = F.cross_entropy(logits, yb_mod)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step: {step}, loss: {loss.item():.4f}\")"
   ],
   "id": "2e61ac3b7e2ca5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.3422\n",
      "step: 100, loss: 1.2428\n",
      "step: 200, loss: 0.2845\n",
      "step: 300, loss: 0.0815\n",
      "step: 400, loss: 0.0465\n",
      "step: 500, loss: 0.0345\n",
      "step: 600, loss: 0.0179\n",
      "step: 700, loss: 0.0136\n",
      "step: 800, loss: 0.0166\n",
      "step: 900, loss: 0.0091\n",
      "step: 1000, loss: 0.0075\n",
      "step: 1100, loss: 0.0054\n",
      "step: 1200, loss: 0.0048\n",
      "step: 1300, loss: 0.0039\n",
      "step: 1400, loss: 0.0029\n",
      "step: 1500, loss: 0.0034\n",
      "step: 1600, loss: 0.0046\n",
      "step: 1700, loss: 0.0024\n",
      "step: 1800, loss: 0.0019\n",
      "step: 1900, loss: 0.0018\n",
      "step: 2000, loss: 0.0018\n",
      "step: 2100, loss: 0.0014\n",
      "step: 2200, loss: 0.0014\n",
      "step: 2300, loss: 0.0010\n",
      "step: 2400, loss: 0.0010\n",
      "step: 2500, loss: 0.0009\n",
      "step: 2600, loss: 0.0009\n",
      "step: 2700, loss: 0.0008\n",
      "step: 2800, loss: 0.0008\n",
      "step: 2900, loss: 0.0007\n",
      "step: 3000, loss: 0.0007\n",
      "step: 3100, loss: 0.0007\n",
      "step: 3200, loss: 0.0006\n",
      "step: 3300, loss: 0.0006\n",
      "step: 3400, loss: 0.0004\n",
      "step: 3500, loss: 0.0004\n",
      "step: 3600, loss: 0.0004\n",
      "step: 3700, loss: 0.0004\n",
      "step: 3800, loss: 0.0004\n",
      "step: 3900, loss: 0.0004\n",
      "step: 4000, loss: 0.0003\n",
      "step: 4100, loss: 0.0003\n",
      "step: 4200, loss: 0.0003\n",
      "step: 4300, loss: 0.0003\n",
      "step: 4400, loss: 0.0003\n",
      "step: 4500, loss: 0.0003\n",
      "step: 4600, loss: 0.0002\n",
      "step: 4700, loss: 0.0003\n",
      "step: 4800, loss: 0.0002\n",
      "step: 4900, loss: 0.0002\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_validation_batch(batch_size, split='val'):\n",
    "    x, y = {\n",
    "        'train': (xtrain, ytrain),\n",
    "        'val': (xval, yval),\n",
    "        'test': (xtest, ytest)\n",
    "    }[split]\n",
    "\n",
    "    iter_size = len(x) // batch_size\n",
    "    for i in range(iter_size):\n",
    "        idx = torch.arange(i*batch_size, i*batch_size + batch_size)\n",
    "        xb = [encode(x[i]) for i in idx]\n",
    "        yidx = [y[i] for i in idx]\n",
    "        phoneme_sequences = [encode_phonemes(p.split(' ')) for p in yidx]\n",
    "        yb, p_lengths = pad_sequences(phoneme_sequences)\n",
    "        xb, lengths = pad_sequences(xb)\n",
    "        yield xb, yb, lengths, p_lengths\n",
    "\n",
    "def compute_accuracy():\n",
    "    pad_index = stoi['<pad>']\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb, lengths, p_lengths in get_validation_batch(batch_size, 'val'):\n",
    "        xb, yb, lengths, p_lengths = xb.to(device), yb.to(device), lengths.to(device), p_lengths.to(device)\n",
    "        logits = model(xb, lengths, yb, p_lengths)\n",
    "        preds = torch.argmax(logits, dim=2)\n",
    "        print(f\"Preds:    \", preds)\n",
    "        print(f\"Targets:  \", yb)\n",
    "        match = (preds == yb) | (yb == pad_index)  # ignore padding\n",
    "        sequence_match = match.all(dim=1)          # all tokens in the row must match\n",
    "        correct += sequence_match.sum().item()\n",
    "        total += sequence_match.size(0)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "val_acc = compute_accuracy()"
   ],
   "id": "eae206f1af1351ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T09:35:24.522504Z",
     "start_time": "2025-05-17T09:35:24.469195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "a = model.decoder.aw[0].unsqueeze(1)\n",
    "sns.heatmap(a.detach().cpu().numpy(), cmap=\"viridis\")\n",
    "plt.xlabel(\"Input Characters\")\n",
    "plt.ylabel(\"Decoder Steps\")\n",
    "plt.show()"
   ],
   "id": "29bc7539e0f2ed05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d966d9fb57a8f60f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
