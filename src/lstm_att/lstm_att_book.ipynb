{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T19:02:57.252237Z",
     "start_time": "2025-05-15T19:02:57.236041Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from lstm_att.lstm_attention import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:02:59.450999Z",
     "start_time": "2025-05-15T19:02:59.440242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data preparation\n",
    "dataset_dir = '/Users/subhojit/datasets/sms_spam_collection'\n",
    "df = pd.read_csv(dataset_dir + \"/SMSSpamCollection\", sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "chars = sorted(set(''.join(texts)))\n",
    "stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "stoi['<PAD>'] = 0\n",
    "vocab_size = len(stoi)\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "\n",
    "xtrain, xval, ytrain, yval = train_test_split(texts, labels, test_size=0.2, random_state=1894)"
   ],
   "id": "ff7933e94c0eb6b7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:03:08.428867Z",
     "start_time": "2025-05-15T19:03:08.424899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_sequences(sequences, max_len=256):\n",
    "    padded = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    lengths = torch.zeros(len(sequences), dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq = seq[:max_len]\n",
    "        padded[i, :len(seq)] = torch.tensor(seq)\n",
    "        lengths[i] = len(seq)\n",
    "    return padded, lengths\n",
    "\n",
    "def get_batch(batch_size, split='train'):\n",
    "    x = xtrain if split == 'train' else xval\n",
    "    y = ytrain if split == 'train' else yval\n",
    "    idx = torch.randint(0, len(x), (batch_size,))\n",
    "    xb = [encode(x[i]) for i in idx]\n",
    "    yb = [y[i] for i in idx]\n",
    "    xb, lengths = pad_sequences(xb)\n",
    "    return xb, torch.tensor(yb, dtype=torch.long), lengths\n"
   ],
   "id": "ac33059fa52aa0a4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:19:42.942499Z",
     "start_time": "2025-05-15T19:19:42.938838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 32\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "batch_size = 256\n",
    "attention_dim = 64\n",
    "seq_len = 10\n",
    "learning_rate = 1e-2\n",
    "max_iter = 1500\n",
    "eval_interval = 500\n",
    "device = \"mps\""
   ],
   "id": "e55fd9ec117b33b2",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:19:44.038878Z",
     "start_time": "2025-05-15T19:19:44.033938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lets_train():\n",
    "    model = LSTMAndAdditiveAttention(vocab_size, hidden_size, output_size, embedding_dim, attention_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        xb, yb, lengths = get_batch(batch_size, split='train')\n",
    "        xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "\n",
    "        logits, context = model(xb, lengths)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            print(f\"Step {step}, loss = {loss.item():.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "7b3fa86ed0b05165",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:19:45.605184Z",
     "start_time": "2025-05-15T19:19:45.598738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_validation_batch(batch_size, split='val'):\n",
    "    x = xtrain if split == 'train' else xval\n",
    "    y = ytrain if split == 'train' else yval\n",
    "\n",
    "    iter_size = len(x) // batch_size\n",
    "    for i in range(iter_size):\n",
    "        idx = torch.arange(i*batch_size, i*batch_size + batch_size)\n",
    "        xb = [encode(x[i]) for i in idx]\n",
    "        yb = [y[i] for i in idx]\n",
    "        xb, lengths = pad_sequences(xb)\n",
    "        yb = torch.tensor(yb, dtype=torch.long)\n",
    "        yield xb, yb, lengths\n",
    "\n",
    "def compute_accuracy(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb, lengths in get_validation_batch(batch_size):\n",
    "        xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
    "        logits, _ = model(xb, lengths)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += len(yb)\n",
    "        # print(\"Preds:   \", preds.tolist())\n",
    "        # print(\"Targets: \", yb.tolist())\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    model.train()\n"
   ],
   "id": "8477dd660a9a4ac2",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:21:10.065402Z",
     "start_time": "2025-05-15T19:19:46.651222Z"
    }
   },
   "cell_type": "code",
   "source": "model = lets_train()\n",
   "id": "991ee7d4579df3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 0.6938\n",
      "Step 500, loss = 0.0031\n",
      "Step 1000, loss = 0.0001\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:15:31.905169Z",
     "start_time": "2025-05-15T19:15:31.898679Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "e8631fb4cb6bf9f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMAndAdditiveAttention(\n",
       "  (embedding): Embedding(119, 32, padding_idx=0)\n",
       "  (lstm): LSTM(32, 256, batch_first=True)\n",
       "  (attention): DRawAdditiveAttention()\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:19:29.039646Z",
     "start_time": "2025-05-15T19:19:28.276986Z"
    }
   },
   "cell_type": "code",
   "source": "compute_accuracy(model)",
   "id": "676a100c5c631b8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.7539\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf86d19ca5f07f11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
