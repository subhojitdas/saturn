{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from lstm.lstm_batch_emb import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "d105048348691dba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_names(filename):\n",
    "    names = open(filename, 'r').read().splitlines()\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "    special_chars = ['<PAD>', '<SOS>', '<EOS>']\n",
    "    all_chars = special_chars + chars\n",
    "    stoi = {ch: i for i, ch in enumerate(all_chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(all_chars)}\n",
    "    vocab_size = len(stoi)\n",
    "    return stoi, itos, vocab_size, names\n",
    "\n",
    "stoi, itos, vocab_size, names = load_names('indian_names.txt')\n",
    "stoi"
   ],
   "id": "91aa030fa8b43734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def name_to_sequence(name):\n",
    "    input_seq = ['<SOS>'] + list(name)\n",
    "    output_seq = list(name) + ['<EOS>']\n",
    "    input_idx = [stoi[i] for i in input_seq]\n",
    "    output_idx = [stoi[i] for i in output_seq]\n",
    "\n",
    "    return input_idx, output_idx\n",
    "\n",
    "def pad_sequences(sequences):\n",
    "    pad_idx = stoi['<PAD>']\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = np.full((max_len, len(sequences)), pad_idx, dtype=np.int32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[:len(seq), i] = seq\n",
    "    return padded\n",
    "\n",
    "ws = [name_to_sequence(name) for name in names[:10]]\n",
    "ws"
   ],
   "id": "76ed3cb8748ebc8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_batches(X_data, Y_data, batch_size):\n",
    "    for i in range(0, len(X_data), batch_size):\n",
    "        batch_X = X_data[i:i+batch_size]\n",
    "        batch_Y = Y_data[i:i+batch_size]\n",
    "        x_padded = pad_sequences(batch_X)\n",
    "        y_padded = pad_sequences(batch_Y)\n",
    "        yield x_padded, y_padded"
   ],
   "id": "f8a63f3d11359ec3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vectorized_loss_and_gradient_batched(outputs, target_idxs, pad_idx):\n",
    "    \"\"\"\n",
    "    outputs: list of (yt, ht, ct), each yt is (output_size, batch_size)\n",
    "    target_idxs: (seq_len, batch_size), integers\n",
    "    pad_idx: index of the <PAD> token\n",
    "    dy_list: list of gradients for each yt\n",
    "    \"\"\"\n",
    "    seq_len, batch_size = target_idxs.shape\n",
    "\n",
    "    Y_logits = np.stack([yt for (yt, _, _) in outputs], axis=0) # (seq_len, output_size, batch_size)\n",
    "\n",
    "    Y_logits_shifted = Y_logits - np.max(Y_logits, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(Y_logits_shifted)\n",
    "    Y_probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Prepare loss and gradient\n",
    "    total_loss = 0.0\n",
    "    dy_list = []\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        dy = Y_probs[t].copy()  # (output_size, batch_size)\n",
    "        for b in range(batch_size):\n",
    "            target = target_idxs[t, b]\n",
    "            if target == pad_idx:\n",
    "                dy[:, b] = 0  # No loss or gradient for PAD\n",
    "            else:\n",
    "                total_loss += -np.log(Y_probs[t, target, b] + 1e-12)\n",
    "                dy[target, b] -= 1\n",
    "        dy_list.append(dy)\n",
    "\n",
    "    return total_loss, dy_list"
   ],
   "id": "6da74ac8dd059664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden_size = 128\n",
    "input_size = vocab_size\n",
    "output_size = vocab_size\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "model = LSTMLayerBatchEmb(input_size, hidden_size, output_size, embedding_dim=embedding_dim)\n",
    "\n",
    "X_data, Y_data = zip(*[name_to_sequence(name) for name in names])\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for xpadded, ypadded in get_batches(X_data, Y_data, batch_size=batch_size):\n",
    "        # (seq_len, batch_size)\n",
    "        seq_len, bsz = xpadded.shape\n",
    "        h0 = np.zeros((hidden_size, bsz))\n",
    "        c0 = np.zeros((hidden_size, bsz))\n",
    "\n",
    "        outputs = model.forward(xpadded, h0, c0)\n",
    "        loss, dy_list = vectorized_loss_and_gradient_batched(outputs, ypadded, 0)\n",
    "        grads, dWy, dby, dembedding = model.backward(dy_list)\n",
    "\n",
    "        lr = 0.1\n",
    "        model.update_parameters(grads, dWy, dby, dembedding, lr)\n",
    "        total_loss += loss\n",
    "        valid_tokens = np.sum(ypadded != 0)\n",
    "        total_loss += loss / valid_tokens\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Avg Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "fef505de713695e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:50:04.088804Z",
     "start_time": "2025-05-05T16:50:04.084053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=0):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "def sample(start_char='<SOS>', max_length=20, temperature=1.0):\n",
    "    model.lstm_batch_cell.cache = None  # Clear cache\n",
    "\n",
    "    # Initialize hidden and cell state\n",
    "    h = np.zeros((model.hidden_size, 1))\n",
    "    c = np.zeros((model.hidden_size, 1))\n",
    "\n",
    "    current_char = start_char\n",
    "    result = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        idx = stoi.get(current_char, stoi[\"<PAD>\"])  # fallback to <PAD>\n",
    "        x = model.embedding[idx].reshape(-1, 1)  # (embedding_dim, 1)\n",
    "\n",
    "        # Forward step\n",
    "        h, c = model.lstm_batch_cell.forward(x, h, c)\n",
    "        y = model.Wy @ h + model.by  # (vocab_size, 1)\n",
    "\n",
    "        # Temperature sampling\n",
    "        probs = softmax(y / temperature, axis=0)\n",
    "        next_idx = np.random.choice(len(probs), p=probs.ravel())\n",
    "        current_char = itos[next_idx]\n",
    "        if current_char == '<EOS>':\n",
    "            break\n",
    "        result.append(current_char)\n",
    "\n",
    "    return ''.join(result)\n"
   ],
   "id": "23437b797fbb88da",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:50:57.918184Z",
     "start_time": "2025-05-05T16:50:57.907790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _ in range(10):\n",
    "    n = sample()\n",
    "    is_new = False if n in names else True\n",
    "    print(f\"name: {n}, is_new: {is_new}\")"
   ],
   "id": "bdc8bee255c554f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: zenab, is_new: False\n",
      "name: mustikari, is_new: True\n",
      "name: yourenka, is_new: True\n",
      "name: yusub, is_new: True\n",
      "name: juwa, is_new: True\n",
      "name: yuvinderjp, is_new: True\n",
      "name: zunh, is_new: True\n",
      "name: tundiram, is_new: False\n",
      "name: tuskh, is_new: True\n",
      "name: wazid, is_new: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_54895/1351133546.py:23: RuntimeWarning: divide by zero encountered in matmul\n",
      "  y = model.Wy @ h + model.by  # (vocab_size, 1)\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_54895/1351133546.py:23: RuntimeWarning: overflow encountered in matmul\n",
      "  y = model.Wy @ h + model.by  # (vocab_size, 1)\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_54895/1351133546.py:23: RuntimeWarning: invalid value encountered in matmul\n",
      "  y = model.Wy @ h + model.by  # (vocab_size, 1)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ada8e36c94081b9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
