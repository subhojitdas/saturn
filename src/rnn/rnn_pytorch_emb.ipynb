{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-28T15:17:55.956956Z",
     "start_time": "2025-04-28T15:14:23.502452Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ---- Setup ----\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Vocabulary: a-z + special tokens\n",
    "all_characters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "special_tokens = ['<SOS>', '<EOS>']\n",
    "all_tokens = special_tokens + list(all_characters)\n",
    "n_characters = len(all_tokens)\n",
    "\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(all_tokens)}\n",
    "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "\n",
    "SOS_idx = char_to_idx['<SOS>']\n",
    "EOS_idx = char_to_idx['<EOS>']\n",
    "\n",
    "# Helpers\n",
    "def string_to_tensor(name):\n",
    "    indices = [char_to_idx[c] for c in name]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "def tensor_to_string(tensor):\n",
    "    chars = [idx_to_char[idx.item()] for idx in tensor]\n",
    "    return ''.join(chars)\n",
    "\n",
    "# ---- Model ----\n",
    "\n",
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(NameRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRUCell(embedding_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_idx, hidden):\n",
    "        embedded = self.embedding(input_idx)\n",
    "        hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# ---- Instantiate ----\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_size = 512\n",
    "rnn = NameRNN(n_characters, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.003)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3000, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---- Data Preparation ----\n",
    "\n",
    "# Your real dataset (replace this with your full 6000 names list)\n",
    "training_data = open('indian_names.txt', 'r').read().splitlines()\n",
    "\n",
    "def prepare_batch(names, batch_size):\n",
    "    batch_inputs = []\n",
    "    batch_targets = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        name = random.choice(names)\n",
    "        name = ['<SOS>'] + list(name) + ['<EOS>']\n",
    "\n",
    "        input_seq = [char_to_idx[ch] for ch in name[:-1]]\n",
    "        target_seq = [char_to_idx[ch] for ch in name[1:]]\n",
    "\n",
    "        batch_inputs.append(torch.tensor(input_seq, dtype=torch.long))\n",
    "        batch_targets.append(torch.tensor(target_seq, dtype=torch.long))\n",
    "\n",
    "    # Pad sequences to same length\n",
    "    input_lengths = [len(seq) for seq in batch_inputs]\n",
    "    max_len = max(input_lengths)\n",
    "\n",
    "    padded_inputs = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    padded_targets = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        padded_inputs[i, :input_lengths[i]] = batch_inputs[i]\n",
    "        padded_targets[i, :input_lengths[i]] = batch_targets[i]\n",
    "\n",
    "    return padded_inputs.to(device), padded_targets.to(device), input_lengths\n",
    "\n",
    "# ---- Training ----\n",
    "\n",
    "def train_step(batch_inputs, batch_targets, input_lengths):\n",
    "    rnn.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_size, seq_len = batch_inputs.shape\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(seq_len):\n",
    "        input_t = batch_inputs[:, t]\n",
    "        target_t = batch_targets[:, t]\n",
    "\n",
    "        output, hidden = rnn(input_t, hidden)\n",
    "        loss += criterion(output, target_t)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss.item() / seq_len\n",
    "\n",
    "# ---- Sampling ----\n",
    "\n",
    "def sample(start_letter='<SOS>', temperature=0.8, max_length=20):\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        if start_letter == '<SOS>':\n",
    "            input_idx = torch.tensor([SOS_idx], device=device)\n",
    "        else:\n",
    "            input_idx = torch.tensor([char_to_idx[start_letter]], device=device)\n",
    "\n",
    "        hidden = rnn.init_hidden(1)\n",
    "\n",
    "        output_name = ''\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = rnn(input_idx, hidden)\n",
    "\n",
    "            output = output.view(-1) / temperature\n",
    "            probs = F.softmax(output, dim=0)\n",
    "            top_idx = torch.multinomial(probs, 1)[0]\n",
    "\n",
    "            predicted_char = idx_to_char[top_idx.item()]\n",
    "\n",
    "            if predicted_char == '<EOS>':\n",
    "                break\n",
    "\n",
    "            output_name += predicted_char\n",
    "            input_idx = top_idx.unsqueeze(0)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# ---- Main Training Loop ----\n",
    "\n",
    "n_epochs = 20000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    batch_inputs, batch_targets, input_lengths = prepare_batch(training_data, batch_size)\n",
    "    loss = train_step(batch_inputs, batch_targets, input_lengths)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        for _ in range(3):\n",
    "            print(f\"Generated (0.8): {sample(temperature=0.8)}\")\n",
    "            print(f\"Generated (1.2): {sample(temperature=1.2)}\")\n",
    "        print()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 500, Loss: 1.1389\n",
      "Generated (0.8): gurdar\n",
      "Generated (1.2): alam\n",
      "Generated (0.8): yogesh\n",
      "Generated (1.2): raghuvirki\n",
      "Generated (0.8): dwarika\n",
      "Generated (1.2): ganhu\n",
      "\n",
      "Epoch 1000, Loss: 0.9251\n",
      "Generated (0.8): manpreet\n",
      "Generated (1.2): reetu\n",
      "Generated (0.8): jairam\n",
      "Generated (1.2): sant\n",
      "Generated (0.8): amira\n",
      "Generated (1.2): sarmeela\n",
      "\n",
      "Epoch 1500, Loss: 1.0169\n",
      "Generated (0.8): saista\n",
      "Generated (1.2): rajshre\n",
      "Generated (0.8): samrin\n",
      "Generated (1.2): murlesh\n",
      "Generated (0.8): pregpal\n",
      "Generated (1.2): pintish\n",
      "\n",
      "Epoch 2000, Loss: 1.0870\n",
      "Generated (0.8): subhod\n",
      "Generated (1.2): teku\n",
      "Generated (0.8): munna\n",
      "Generated (1.2): iqarat\n",
      "Generated (0.8): mangla\n",
      "Generated (1.2): dimpu\n",
      "\n",
      "Epoch 2500, Loss: 1.1074\n",
      "Generated (0.8): awedhesh\n",
      "Generated (1.2): akash\n",
      "Generated (0.8): chotum\n",
      "Generated (1.2): ramkaram\n",
      "Generated (0.8): kalpesh\n",
      "Generated (1.2): nnan\n",
      "\n",
      "Epoch 3000, Loss: 0.9184\n",
      "Generated (0.8): sumanthra\n",
      "Generated (1.2): rehal\n",
      "Generated (0.8): nasar\n",
      "Generated (1.2): zineesh\n",
      "Generated (0.8): muskarn\n",
      "Generated (1.2): bundu\n",
      "\n",
      "Epoch 3500, Loss: 0.8807\n",
      "Generated (0.8): changa\n",
      "Generated (1.2): jarjeet\n",
      "Generated (0.8): saru\n",
      "Generated (1.2): nosi\n",
      "Generated (0.8): kaloo\n",
      "Generated (1.2): alaman\n",
      "\n",
      "Epoch 4000, Loss: 1.0502\n",
      "Generated (0.8): sukkhu\n",
      "Generated (1.2): sadab\n",
      "Generated (0.8): ashni\n",
      "Generated (1.2): mamtha\n",
      "Generated (0.8): kamana\n",
      "Generated (1.2): sarangthem\n",
      "\n",
      "Epoch 4500, Loss: 0.7996\n",
      "Generated (0.8): minakshi\n",
      "Generated (1.2): roshini\n",
      "Generated (0.8): ranveer\n",
      "Generated (1.2): rfhala\n",
      "Generated (0.8): saraswati\n",
      "Generated (1.2): naresh\n",
      "\n",
      "Epoch 5000, Loss: 0.9600\n",
      "Generated (0.8): bhateri\n",
      "Generated (1.2): uda\n",
      "Generated (0.8): sushama\n",
      "Generated (1.2): nirpha\n",
      "Generated (0.8): asfak\n",
      "Generated (1.2): mamina\n",
      "\n",
      "Epoch 5500, Loss: 0.9322\n",
      "Generated (0.8): akhtari\n",
      "Generated (1.2): kauslender\n",
      "Generated (0.8): sidhi\n",
      "Generated (1.2): ranju\n",
      "Generated (0.8): mohni\n",
      "Generated (1.2): dara\n",
      "\n",
      "Epoch 6000, Loss: 0.8700\n",
      "Generated (0.8): priyanka\n",
      "Generated (1.2): bake\n",
      "Generated (0.8): jasveena\n",
      "Generated (1.2): abrar\n",
      "Generated (0.8): vanshu\n",
      "Generated (1.2): khukankana\n",
      "\n",
      "Epoch 6500, Loss: 0.7064\n",
      "Generated (0.8): chandracala\n",
      "Generated (1.2): julekha\n",
      "Generated (0.8): jalsingh\n",
      "Generated (1.2): ramsewak\n",
      "Generated (0.8): tarun\n",
      "Generated (1.2): sukhvir\n",
      "\n",
      "Epoch 7000, Loss: 1.0349\n",
      "Generated (0.8): sayari\n",
      "Generated (1.2): tavinder\n",
      "Generated (0.8): rashika\n",
      "Generated (1.2): rasli\n",
      "Generated (0.8): jeeya\n",
      "Generated (1.2): sayra\n",
      "\n",
      "Epoch 7500, Loss: 0.8194\n",
      "Generated (0.8): chandan\n",
      "Generated (1.2): mujli\n",
      "Generated (0.8): dhramender\n",
      "Generated (1.2): asmin\n",
      "Generated (0.8): ranveer\n",
      "Generated (1.2): rajni\n",
      "\n",
      "Epoch 8000, Loss: 0.9296\n",
      "Generated (0.8): gendi\n",
      "Generated (1.2): samsida\n",
      "Generated (0.8): rupendra\n",
      "Generated (1.2): julee\n",
      "Generated (0.8): jon\n",
      "Generated (1.2): avnit\n",
      "\n",
      "Epoch 8500, Loss: 0.8281\n",
      "Generated (0.8): rachpreet\n",
      "Generated (1.2): tunni\n",
      "Generated (0.8): janak\n",
      "Generated (1.2): ruksar\n",
      "Generated (0.8): parshant\n",
      "Generated (1.2): balram\n",
      "\n",
      "Epoch 9000, Loss: 0.7481\n",
      "Generated (0.8): veerpal\n",
      "Generated (1.2): joycee\n",
      "Generated (0.8): harbas\n",
      "Generated (1.2): saibul\n",
      "Generated (0.8): kulwinder\n",
      "Generated (1.2): tara\n",
      "\n",
      "Epoch 9500, Loss: 0.8200\n",
      "Generated (0.8): umlesh\n",
      "Generated (1.2): avastha\n",
      "Generated (0.8): armita\n",
      "Generated (1.2): visham\n",
      "Generated (0.8): radheshyam\n",
      "Generated (1.2): shreee\n",
      "\n",
      "Epoch 10000, Loss: 0.9058\n",
      "Generated (0.8): juga\n",
      "Generated (1.2): kola\n",
      "Generated (0.8): jalaluddin\n",
      "Generated (1.2): bagaram\n",
      "Generated (0.8): sadab\n",
      "Generated (1.2): ramtek\n",
      "\n",
      "Epoch 10500, Loss: 0.8230\n",
      "Generated (0.8): kalicharan\n",
      "Generated (1.2): alma\n",
      "Generated (0.8): silpa\n",
      "Generated (1.2): sefali\n",
      "Generated (0.8): dakshya\n",
      "Generated (1.2): shardha\n",
      "\n",
      "Epoch 11000, Loss: 0.7463\n",
      "Generated (0.8): angreg\n",
      "Generated (1.2): prahlad\n",
      "Generated (0.8): gajwanti\n",
      "Generated (1.2): nagma\n",
      "Generated (0.8): akhalak\n",
      "Generated (1.2): havbasha\n",
      "\n",
      "Epoch 11500, Loss: 0.6838\n",
      "Generated (0.8): panni\n",
      "Generated (1.2): rimasa\n",
      "Generated (0.8): sohan\n",
      "Generated (1.2): shveta\n",
      "Generated (0.8): anaro\n",
      "Generated (1.2): nifaya\n",
      "\n",
      "Epoch 12000, Loss: 0.6960\n",
      "Generated (0.8): sushree\n",
      "Generated (1.2): gomati\n",
      "Generated (0.8): paramjeet\n",
      "Generated (1.2): azruddin\n",
      "Generated (0.8): shanshak\n",
      "Generated (1.2): shaira\n",
      "\n",
      "Epoch 12500, Loss: 0.8872\n",
      "Generated (0.8): veerendra\n",
      "Generated (1.2): kudtusu\n",
      "Generated (0.8): bannu\n",
      "Generated (1.2): nuli\n",
      "Generated (0.8): ramgopal\n",
      "Generated (1.2): khursid\n",
      "\n",
      "Epoch 13000, Loss: 0.8188\n",
      "Generated (0.8): manvendra\n",
      "Generated (1.2): shivcharan\n",
      "Generated (0.8): dharma\n",
      "Generated (1.2): bharatlal\n",
      "Generated (0.8): trijugi\n",
      "Generated (1.2): ankus\n",
      "\n",
      "Epoch 13500, Loss: 0.8943\n",
      "Generated (0.8): nency\n",
      "Generated (1.2): bhakvareen\n",
      "Generated (0.8): sahishta\n",
      "Generated (1.2): veera\n",
      "Generated (0.8): parven\n",
      "Generated (1.2): shahmina\n",
      "\n",
      "Epoch 14000, Loss: 0.8040\n",
      "Generated (0.8): rajeena\n",
      "Generated (1.2): chhabeel\n",
      "Generated (0.8): lalu\n",
      "Generated (1.2): kirshna\n",
      "Generated (0.8): naisi\n",
      "Generated (1.2): jodhi\n",
      "\n",
      "Epoch 14500, Loss: 0.7979\n",
      "Generated (0.8): ramswaroop\n",
      "Generated (1.2): islamudin\n",
      "Generated (0.8): rajenderi\n",
      "Generated (1.2): raina\n",
      "Generated (0.8): jasleen\n",
      "Generated (1.2): lalit\n",
      "\n",
      "Epoch 15000, Loss: 0.7997\n",
      "Generated (0.8): najiya\n",
      "Generated (1.2): laxminarain\n",
      "Generated (0.8): sandeepa\n",
      "Generated (1.2): sonika\n",
      "Generated (0.8): saurav\n",
      "Generated (1.2): lukman\n",
      "\n",
      "Epoch 15500, Loss: 0.9878\n",
      "Generated (0.8): sahabudeen\n",
      "Generated (1.2): abishak\n",
      "Generated (0.8): udaiy\n",
      "Generated (1.2): sanju\n",
      "Generated (0.8): parmo\n",
      "Generated (1.2): meenat\n",
      "\n",
      "Epoch 16000, Loss: 0.8946\n",
      "Generated (0.8): dhirender\n",
      "Generated (1.2): devi\n",
      "Generated (0.8): saraswati\n",
      "Generated (1.2): chom\n",
      "Generated (0.8): gulfasa\n",
      "Generated (1.2): attar\n",
      "\n",
      "Epoch 16500, Loss: 0.8104\n",
      "Generated (0.8): kalashi\n",
      "Generated (1.2): mikel\n",
      "Generated (0.8): sangeeta\n",
      "Generated (1.2): prem\n",
      "Generated (0.8): mubarik\n",
      "Generated (1.2): teja\n",
      "\n",
      "Epoch 17000, Loss: 0.8839\n",
      "Generated (0.8): meesh\n",
      "Generated (1.2): jarnan\n",
      "Generated (0.8): sidharth\n",
      "Generated (1.2): jhora\n",
      "Generated (0.8): japneet\n",
      "Generated (1.2): aaraish\n",
      "\n",
      "Epoch 17500, Loss: 0.8100\n",
      "Generated (0.8): lakhbir\n",
      "Generated (1.2): prateek\n",
      "Generated (0.8): tapas\n",
      "Generated (1.2): devdhari\n",
      "Generated (0.8): makbul\n",
      "Generated (1.2): phoolo\n",
      "\n",
      "Epoch 18000, Loss: 0.8869\n",
      "Generated (0.8): jarif\n",
      "Generated (1.2): ritu\n",
      "Generated (0.8): ratan\n",
      "Generated (1.2): syamsunder\n",
      "Generated (0.8): bhupendar\n",
      "Generated (1.2): jhuhi\n",
      "\n",
      "Epoch 18500, Loss: 0.8015\n",
      "Generated (0.8): bismilla\n",
      "Generated (1.2): azmira\n",
      "Generated (0.8): baga\n",
      "Generated (1.2): lur\n",
      "Generated (0.8): asharani\n",
      "Generated (1.2): grrar\n",
      "\n",
      "Epoch 19000, Loss: 0.8096\n",
      "Generated (0.8): moham\n",
      "Generated (1.2): rajeshwar\n",
      "Generated (0.8): shahnaj\n",
      "Generated (1.2): yukilal\n",
      "Generated (0.8): paramjit\n",
      "Generated (1.2): reeti\n",
      "\n",
      "Epoch 19500, Loss: 0.6283\n",
      "Generated (0.8): aashish\n",
      "Generated (1.2): yamini\n",
      "Generated (0.8): aaysha\n",
      "Generated (1.2): dataterya\n",
      "Generated (0.8): beeram\n",
      "Generated (1.2): minakchi\n",
      "\n",
      "Epoch 20000, Loss: 0.9801\n",
      "Generated (0.8): parvinder\n",
      "Generated (1.2): kauslender\n",
      "Generated (0.8): sazia\n",
      "Generated (1.2): neeta\n",
      "Generated (0.8): jhuma\n",
      "Generated (1.2): yespal\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65740ef1d8084222"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
