{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-28T13:12:05.144499Z",
     "start_time": "2025-04-28T13:12:04.899458Z"
    }
   },
   "source": [
    "import os, sys\n",
    "\n",
    "project_root = os.path.abspath('/Users/subhojit/workspace/saturn/src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from rnn.recnet_batch import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T13:12:05.151526Z",
     "start_time": "2025-04-28T13:12:05.147509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('indian_names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chars = ['<PAD>', '<SOS>', '<EOS>'] + chars\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(stoi)\n",
    "print(f\"vocab: {stoi}\")\n",
    "print(max(len(w) for w in words))"
   ],
   "id": "c8c0046703c55320",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n",
      "19\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T13:12:05.215309Z",
     "start_time": "2025-04-28T13:12:05.212017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_name(name):\n",
    "    encode = [stoi['<SOS>']] + [stoi[ch] for ch in name] + [stoi['<EOS>']]\n",
    "    return encode\n",
    "\n",
    "def decode_name(indices):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "encode_name('bobby')"
   ],
   "id": "c6b8c6914717ef52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 17, 4, 4, 27, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T13:12:05.266406Z",
     "start_time": "2025-04-28T13:12:05.220074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_data = []\n",
    "\n",
    "np.random.shuffle(words)\n",
    "\n",
    "for name in words:\n",
    "    # print(name)\n",
    "    encoded = encode_name(name)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(encoded) - 1):\n",
    "        x_idx = encoded[i]\n",
    "        y_idx = encoded[i+1]\n",
    "        x_onehot = np.zeros((vocab_size, 1))\n",
    "        x_onehot[x_idx] = 1.0\n",
    "        inputs.append(x_onehot)\n",
    "        targets.append(y_idx)\n",
    "\n",
    "    training_data.append((inputs, targets))\n",
    "\n",
    "print(len(training_data))"
   ],
   "id": "65fde00156631fa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6485\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T13:15:55.864695Z",
     "start_time": "2025-04-28T13:15:45.065307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_inputs(sequences, pad_vector):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        padded_seq = seq + [pad_vector] * (max_len - len(seq))\n",
    "        padded.append(padded_seq)\n",
    "\n",
    "    return np.array(padded)\n",
    "\n",
    "def pad_target(sequences, pad_idx):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        padded_seq = seq + [pad_idx] * (max_len - len(seq))\n",
    "        padded.append(padded_seq)\n",
    "\n",
    "    return np.array(padded)\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 32  # Set batch size\n",
    "hidden_size = 128\n",
    "\n",
    "rnn = VanillaBatchRNN(input_size=vocab_size, hidden_size=hidden_size, output_size=vocab_size)\n",
    "\n",
    "PAD_IDX = stoi['<PAD>']\n",
    "pad_vector = np.zeros((vocab_size, 1))\n",
    "pad_vector[PAD_IDX] = 1.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    idx = np.random.randint(0, len(training_data), (batch_size,))\n",
    "    batch = [training_data[i] for i in idx]\n",
    "\n",
    "    batch_inputs, batch_targets = zip(*batch)\n",
    "\n",
    "    # Pad to same length\n",
    "    batch_inputs_padded = pad_inputs(batch_inputs, pad_vector)\n",
    "    batch_targets_padded = pad_target(batch_targets, PAD_IDX)\n",
    "\n",
    "    # Now batch_inputs_padded: (batch_size, seq_len)\n",
    "    # transpose to (seq_len, batch_size)\n",
    "    batch_inputs_tensor = np.transpose(batch_inputs_padded, (1, 0, 2, 3))\n",
    "    batch_targets_tensor = np.transpose(batch_targets_padded, (1, 0))\n",
    "\n",
    "    loss = rnn.train_step(batch_inputs_tensor, batch_targets_tensor)\n",
    "    total_loss += loss\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Total Loss: {total_loss}\")"
   ],
   "id": "9c8f8a0b527d7b6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Total Loss: 15.594780855561952,  Avg Loss: 0.0024\n",
      "Epoch 200, Total Loss: 16.86643580533764,  Avg Loss: 0.0026\n",
      "Epoch 300, Total Loss: 13.867842034878665,  Avg Loss: 0.0021\n",
      "Epoch 400, Total Loss: 14.249396619332645,  Avg Loss: 0.0022\n",
      "Epoch 500, Total Loss: 13.700381300752074,  Avg Loss: 0.0021\n",
      "Epoch 600, Total Loss: 12.748666077413747,  Avg Loss: 0.0020\n",
      "Epoch 700, Total Loss: 14.355873268188835,  Avg Loss: 0.0022\n",
      "Epoch 800, Total Loss: 14.194644605653691,  Avg Loss: 0.0022\n",
      "Epoch 900, Total Loss: 14.567304635142136,  Avg Loss: 0.0022\n",
      "Epoch 1000, Total Loss: 14.613040933857956,  Avg Loss: 0.0023\n",
      "Epoch 1100, Total Loss: 16.58243531330838,  Avg Loss: 0.0026\n",
      "Epoch 1200, Total Loss: 14.158134557314359,  Avg Loss: 0.0022\n",
      "Epoch 1300, Total Loss: 13.868278542025337,  Avg Loss: 0.0021\n",
      "Epoch 1400, Total Loss: 15.525643562364632,  Avg Loss: 0.0024\n",
      "Epoch 1500, Total Loss: 13.8486873154172,  Avg Loss: 0.0021\n",
      "Epoch 1600, Total Loss: 14.818775557494618,  Avg Loss: 0.0023\n",
      "Epoch 1700, Total Loss: 13.867841965550031,  Avg Loss: 0.0021\n",
      "Epoch 1800, Total Loss: 14.481645575554566,  Avg Loss: 0.0022\n",
      "Epoch 1900, Total Loss: 13.969309823112791,  Avg Loss: 0.0022\n",
      "Epoch 2000, Total Loss: 14.218462946956999,  Avg Loss: 0.0022\n",
      "Epoch 2100, Total Loss: 14.505007025452448,  Avg Loss: 0.0022\n",
      "Epoch 2200, Total Loss: 13.482091643067204,  Avg Loss: 0.0021\n",
      "Epoch 2300, Total Loss: 13.606184450259883,  Avg Loss: 0.0021\n",
      "Epoch 2400, Total Loss: 13.838312374735892,  Avg Loss: 0.0021\n",
      "Epoch 2500, Total Loss: 13.911451601057777,  Avg Loss: 0.0021\n",
      "Epoch 2600, Total Loss: 14.944213034690568,  Avg Loss: 0.0023\n",
      "Epoch 2700, Total Loss: 14.150759213128135,  Avg Loss: 0.0022\n",
      "Epoch 2800, Total Loss: 13.982298529359145,  Avg Loss: 0.0022\n",
      "Epoch 2900, Total Loss: 14.522516407214932,  Avg Loss: 0.0022\n",
      "Epoch 3000, Total Loss: 13.795336274432069,  Avg Loss: 0.0021\n",
      "Epoch 3100, Total Loss: 12.728882824500374,  Avg Loss: 0.0020\n",
      "Epoch 3200, Total Loss: 14.102908700086523,  Avg Loss: 0.0022\n",
      "Epoch 3300, Total Loss: 15.7269577197458,  Avg Loss: 0.0024\n",
      "Epoch 3400, Total Loss: 14.712279272826887,  Avg Loss: 0.0023\n",
      "Epoch 3500, Total Loss: 15.632348169693847,  Avg Loss: 0.0024\n",
      "Epoch 3600, Total Loss: 12.944416632310919,  Avg Loss: 0.0020\n",
      "Epoch 3700, Total Loss: 10.578559345730167,  Avg Loss: 0.0016\n",
      "Epoch 3800, Total Loss: 11.828628583530975,  Avg Loss: 0.0018\n",
      "Epoch 3900, Total Loss: 11.202961653480717,  Avg Loss: 0.0017\n",
      "Epoch 4000, Total Loss: 12.842519872845067,  Avg Loss: 0.0020\n",
      "Epoch 4100, Total Loss: 13.299320752152465,  Avg Loss: 0.0021\n",
      "Epoch 4200, Total Loss: 11.48468498812457,  Avg Loss: 0.0018\n",
      "Epoch 4300, Total Loss: 13.412558345259459,  Avg Loss: 0.0021\n",
      "Epoch 4400, Total Loss: 13.281017229288455,  Avg Loss: 0.0020\n",
      "Epoch 4500, Total Loss: 12.29789200429619,  Avg Loss: 0.0019\n",
      "Epoch 4600, Total Loss: 13.46601663553373,  Avg Loss: 0.0021\n",
      "Epoch 4700, Total Loss: 13.254941911747883,  Avg Loss: 0.0020\n",
      "Epoch 4800, Total Loss: 14.18183091136674,  Avg Loss: 0.0022\n",
      "Epoch 4900, Total Loss: 12.83679541019508,  Avg Loss: 0.0020\n",
      "Epoch 5000, Total Loss: 11.87948193626106,  Avg Loss: 0.0018\n",
      "Epoch 5100, Total Loss: 10.959322556387855,  Avg Loss: 0.0017\n",
      "Epoch 5200, Total Loss: 13.80763510530413,  Avg Loss: 0.0021\n",
      "Epoch 5300, Total Loss: 14.082308364498173,  Avg Loss: 0.0022\n",
      "Epoch 5400, Total Loss: 13.053507831684533,  Avg Loss: 0.0020\n",
      "Epoch 5500, Total Loss: 10.697426573679074,  Avg Loss: 0.0016\n",
      "Epoch 5600, Total Loss: 12.838799085210946,  Avg Loss: 0.0020\n",
      "Epoch 5700, Total Loss: 10.885184785322418,  Avg Loss: 0.0017\n",
      "Epoch 5800, Total Loss: 13.225811944513742,  Avg Loss: 0.0020\n",
      "Epoch 5900, Total Loss: 10.635045900872226,  Avg Loss: 0.0016\n",
      "Epoch 6000, Total Loss: 13.075436133287377,  Avg Loss: 0.0020\n",
      "Epoch 6100, Total Loss: 13.095887948331784,  Avg Loss: 0.0020\n",
      "Epoch 6200, Total Loss: 13.501513886548823,  Avg Loss: 0.0021\n",
      "Epoch 6300, Total Loss: 11.063339893239394,  Avg Loss: 0.0017\n",
      "Epoch 6400, Total Loss: 12.835446486787879,  Avg Loss: 0.0020\n",
      "Epoch 6500, Total Loss: 12.904070619000414,  Avg Loss: 0.0020\n",
      "Epoch 6600, Total Loss: 10.472738710993681,  Avg Loss: 0.0016\n",
      "Epoch 6700, Total Loss: 14.333592201669187,  Avg Loss: 0.0022\n",
      "Epoch 6800, Total Loss: 14.794108737095002,  Avg Loss: 0.0023\n",
      "Epoch 6900, Total Loss: 12.575328913967125,  Avg Loss: 0.0019\n",
      "Epoch 7000, Total Loss: 11.669919899464146,  Avg Loss: 0.0018\n",
      "Epoch 7100, Total Loss: 14.024836473122827,  Avg Loss: 0.0022\n",
      "Epoch 7200, Total Loss: 15.01005449711815,  Avg Loss: 0.0023\n",
      "Epoch 7300, Total Loss: 14.443488308326323,  Avg Loss: 0.0022\n",
      "Epoch 7400, Total Loss: 14.370101928716082,  Avg Loss: 0.0022\n",
      "Epoch 7500, Total Loss: 13.753046260403053,  Avg Loss: 0.0021\n",
      "Epoch 7600, Total Loss: 15.163354008928195,  Avg Loss: 0.0023\n",
      "Epoch 7700, Total Loss: 14.44316214991473,  Avg Loss: 0.0022\n",
      "Epoch 7800, Total Loss: 13.527687418683774,  Avg Loss: 0.0021\n",
      "Epoch 7900, Total Loss: 15.540788485957096,  Avg Loss: 0.0024\n",
      "Epoch 8000, Total Loss: 15.590419895496998,  Avg Loss: 0.0024\n",
      "Epoch 8100, Total Loss: 15.486505244238737,  Avg Loss: 0.0024\n",
      "Epoch 8200, Total Loss: 13.9126086575898,  Avg Loss: 0.0021\n",
      "Epoch 8300, Total Loss: 13.22543043055135,  Avg Loss: 0.0020\n",
      "Epoch 8400, Total Loss: 14.814576234425544,  Avg Loss: 0.0023\n",
      "Epoch 8500, Total Loss: 12.740735194366732,  Avg Loss: 0.0020\n",
      "Epoch 8600, Total Loss: 15.01913458185619,  Avg Loss: 0.0023\n",
      "Epoch 8700, Total Loss: 13.911451601057784,  Avg Loss: 0.0021\n",
      "Epoch 8800, Total Loss: 14.684498488140514,  Avg Loss: 0.0023\n",
      "Epoch 8900, Total Loss: 14.098001272764622,  Avg Loss: 0.0022\n",
      "Epoch 9000, Total Loss: 14.372669082834326,  Avg Loss: 0.0022\n",
      "Epoch 9100, Total Loss: 15.399887046664496,  Avg Loss: 0.0024\n",
      "Epoch 9200, Total Loss: 15.257493495781352,  Avg Loss: 0.0024\n",
      "Epoch 9300, Total Loss: 15.456042725766514,  Avg Loss: 0.0024\n",
      "Epoch 9400, Total Loss: 14.286493870195738,  Avg Loss: 0.0022\n",
      "Epoch 9500, Total Loss: 14.83232993053441,  Avg Loss: 0.0023\n",
      "Epoch 9600, Total Loss: 13.719569509870775,  Avg Loss: 0.0021\n",
      "Epoch 9700, Total Loss: 16.326453260141097,  Avg Loss: 0.0025\n",
      "Epoch 9800, Total Loss: 12.472330437535414,  Avg Loss: 0.0019\n",
      "Epoch 9900, Total Loss: 14.678961542276392,  Avg Loss: 0.0023\n",
      "Epoch 10000, Total Loss: 15.164103988228526,  Avg Loss: 0.0023\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1081094df8f1fcbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T13:24:30.620428Z",
     "start_time": "2025-04-28T13:24:30.604858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample(idx_to_char, max_length=20, temperature=1.0):\n",
    "    ht = np.zeros((1, hidden_size))\n",
    "    idx = stoi['<PAD>']\n",
    "    generated_indices = [idx]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        xt = np.zeros((1, vocab_size))\n",
    "        xt[0, idx] = 1  # one-hot input\n",
    "\n",
    "        activation = xt @ rnn.Wxh.T + ht @ rnn.Whh.T + rnn.bh.T\n",
    "        ht = np.tanh(activation)\n",
    "        output_logits = ht @ rnn.Why.T + rnn.by.T\n",
    "\n",
    "        # Temperature scaling\n",
    "        logits = output_logits / temperature\n",
    "\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "\n",
    "        idx = np.random.choice(range(vocab_size), p=probs.ravel())\n",
    "        generated_indices.append(idx)\n",
    "\n",
    "        if idx_to_char[idx] == '\\n':\n",
    "            break\n",
    "\n",
    "    generated_sequence = ''.join(idx_to_char[i] for i in generated_indices)\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "for _ in range(10):\n",
    "    n = sample(itos, temperature=0.5)\n",
    "    print(n)\n"
   ],
   "id": "10deba1f000bf05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n",
      "<PAD>yhsa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa<PAD><PAD>sa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_61693/3203600778.py:10: RuntimeWarning: divide by zero encountered in matmul\n",
      "  activation = xt @ rnn.Wxh.T + ht @ rnn.Whh.T + rnn.bh.T\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_61693/3203600778.py:10: RuntimeWarning: overflow encountered in matmul\n",
      "  activation = xt @ rnn.Wxh.T + ht @ rnn.Whh.T + rnn.bh.T\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_61693/3203600778.py:10: RuntimeWarning: invalid value encountered in matmul\n",
      "  activation = xt @ rnn.Wxh.T + ht @ rnn.Whh.T + rnn.bh.T\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_61693/3203600778.py:12: RuntimeWarning: divide by zero encountered in matmul\n",
      "  output_logits = ht @ rnn.Why.T + rnn.by.T\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_61693/3203600778.py:12: RuntimeWarning: overflow encountered in matmul\n",
      "  output_logits = ht @ rnn.Why.T + rnn.by.T\n",
      "/var/folders/5g/d5t9nrnd47z7b7djysv3l9mm0000gq/T/ipykernel_61693/3203600778.py:12: RuntimeWarning: invalid value encountered in matmul\n",
      "  output_logits = ht @ rnn.Why.T + rnn.by.T\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9cae6f0e78e7713d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
